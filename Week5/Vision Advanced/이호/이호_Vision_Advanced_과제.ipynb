{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ARB6yuk9zpD"
   },
   "source": [
    "## ToBigs 5주차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7NzhIu69zpG"
   },
   "source": [
    "### Vision Advanced 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKlzYdE49zpG"
   },
   "source": [
    "#### 문제 1.\n",
    "\n",
    "Object Detection 에는 2-stage model 과 1-stage model이 존재합니다.  \n",
    "각 유형에 해당하는 model을 하나씩 선정하여 설명하세요. (단, 세션 시간에 설명한 모델과 아래 제시된 모델은 제외할 것)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YZ3sbOc19zpG"
   },
   "source": [
    "### 답안 작성\n",
    "\n",
    "2-stage model : R-FCN(Region-based Fully Convolutional Networks)\n",
    "R-FCN은 투스테이지 모델이지만, 속도와 정확도 사이의 균형을 맞추려고 한 모델.\n",
    "Position-sensitive score maps라는 기법을 도입해 성능을 향상시킴.\n",
    "특징 맵에서 객체의 위치 정보를 보다 세밀하게 처리하여, 보다 빠른 속도와 정확도를 제공하면서도 복잡한 연산을 피함.\n",
    "\n",
    "1-stage model : RetinaNet\n",
    "Focal Loss라는 새로운 손실 함수를 도입하여 일반적인 원스테이지 모델의 약점인 정확도 문제를 해결하려고 한 모델.\n",
    "원스테이지 방식이지만, Focal Loss 덕분에 작은 객체나 어려운 샘플에 대한 탐지 성능을 높였으며, YOLO나 SSD와 비교해 정확도가 더 높은 편.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvw64DOH9zpH"
   },
   "source": [
    "#### 문제 2.\n",
    "\n",
    "아래 제시된 FasterRCNN 과 YOLOv5 를 각각 실행합니다.  \n",
    "실행 결과를 제시하고 두 모델 사이의 차이점을 실행 결과에 근거하여 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k_uN8DZs9zpH"
   },
   "outputs": [],
   "source": [
    "## Package Import\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iK4_kHM1ALw0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "## Data Download\n",
    "\n",
    "import os\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "os.makedirs('./data/images/', exist_ok=True)\n",
    "\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip -d ./data/\n",
    "!wget http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip val2017.zip -d ./data/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://images.cocodataset.org/annotations/annotations_trainval2017.zip to ./data/annotations_trainval2017.zip...\n",
      "Download complete!\n",
      "Unzipping ./data/annotations_trainval2017.zip...\n",
      "Unzip complete!\n",
      "Downloading http://images.cocodataset.org/zips/val2017.zip to ./data/images/val2017.zip...\n",
      "Download complete!\n",
      "Unzipping ./data/images/val2017.zip...\n",
      "Unzip complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# 데이터 디렉토리 생성\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "os.makedirs('./data/images/', exist_ok=True)\n",
    "\n",
    "# 파일 다운로드 함수\n",
    "def download_file(url, dest):\n",
    "    print(f\"Downloading {url} to {dest}...\")\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# 압축 해제 함수\n",
    "def unzip_file(zip_path, extract_to):\n",
    "    print(f\"Unzipping {zip_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(\"Unzip complete!\")\n",
    "\n",
    "# COCO 데이터셋 다운로드 URL\n",
    "annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "images_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "\n",
    "# 다운로드 경로 설정\n",
    "annotations_zip_path = './data/annotations_trainval2017.zip'\n",
    "images_zip_path = './data/images/val2017.zip'\n",
    "\n",
    "# 파일 다운로드 및 압축 해제\n",
    "download_file(annotations_url, annotations_zip_path)\n",
    "unzip_file(annotations_zip_path, './data/')\n",
    "download_file(images_url, images_zip_path)\n",
    "unzip_file(images_zip_path, './data/images/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kQzE9KrL9zpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\이호/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 160M/160M [00:02<00:00, 68.3MB/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x000001F2833F8C10>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     43\u001b[0m         cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x000001F2833F8C10>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "## FasterRCNN\n",
    "\n",
    "# 경로 설정\n",
    "image_dir = \"./data/images/val2017/\"\n",
    "json_path = \"./data/annotations/instances_val2017.json\"\n",
    "\n",
    "# Transform 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset과 DataLoader 설정\n",
    "dataset = CocoDetection(root=image_dir, annFile=json_path, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 모델 로드 및 설정\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 모델의 클래스 수를 3개로 설정 (배경 포함해서 0, 2, background)\n",
    "num_classes = 3  # COCO의 경우 background 포함 (0, 2 두 개의 클래스 + 배경)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# GPU 사용 가능 시 GPU로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 정확도 계산을 위한 변수\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# 소요 시간 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "# 100장 예측 속도를 비교\n",
    "cnt = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in data_loader:\n",
    "        cnt += 1\n",
    "        images = list(image.to(device) for image in images)\n",
    "\n",
    "        # 각 이미지의 타깃을 적절히 변환하여 GPU로 전송\n",
    "        processed_targets = []\n",
    "        for target in targets:\n",
    "            processed_target = {}\n",
    "            processed_target['boxes'] = torch.tensor([ann['bbox'] for ann in target]).to(device)\n",
    "            processed_target['labels'] = torch.tensor([ann['category_id'] for ann in target]).to(device)\n",
    "            processed_targets.append(processed_target)\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            pred_labels = output['labels'].cpu().numpy()\n",
    "            true_labels = processed_targets[i]['labels'].cpu().numpy()\n",
    "\n",
    "            # 예측 수가 실제 라벨 수보다 많은 경우 예측 수를 잘라냄\n",
    "            if len(pred_labels) > len(true_labels):\n",
    "                pred_labels = pred_labels[:len(true_labels)]\n",
    "\n",
    "            # 예측 레이블과 실제 레이블이 얼마나 일치하는지 확인\n",
    "            correct = np.sum(pred_labels == true_labels[:len(pred_labels)])\n",
    "            total_correct += correct\n",
    "            total_predictions += len(true_labels)\n",
    "\n",
    "        if cnt == 100:\n",
    "            break\n",
    "\n",
    "# 소요 시간 측정 종료\n",
    "end_time = time.time()\n",
    "\n",
    "# 소요 시간 및 정확도 출력\n",
    "time_taken = end_time - start_time\n",
    "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"소요 시간: {time_taken:.2f}초\")\n",
    "print(f\"정확도: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stB0A3MN9zpI"
   },
   "outputs": [],
   "source": [
    "## YOLOv5\n",
    "\n",
    "# 경로 설정\n",
    "image_dir = \"./data/images/val2017/\"\n",
    "json_path = \"./data/annotations/instances_val2017.json\"\n",
    "\n",
    "# Transform 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),  # YOLOv5의 기본 입력 크기\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset과 DataLoader 설정\n",
    "dataset = CocoDetection(root=image_dir, annFile=json_path, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "# 모델 로드 (PyTorch Hub에서 COCO로 사전 학습된 YOLOv5 모델 로드)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# GPU 사용 가능 시 GPU로 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 정확도 계산을 위한 변수\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# 소요 시간 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "# 100장 예측 속도를 비교\n",
    "cnt = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in data_loader:\n",
    "        cnt += 1\n",
    "        images = images.to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 예측된 바운딩 박스와 클래스 정보 추출\n",
    "        for output in outputs:\n",
    "            pred_labels = output[:, 5:].argmax(1).cpu().numpy()  # 예측된 클래스 레이블\n",
    "\n",
    "            # 실제 라벨을 추출하고, 예측된 라벨과 비교\n",
    "            true_labels = [t['category_id'].item() for t in targets]\n",
    "\n",
    "            # 예측된 라벨과 실제 라벨이 일치하는지 확인\n",
    "            correct = np.sum(pred_labels[:len(true_labels)] == true_labels)\n",
    "            total_correct += correct\n",
    "            total_predictions += len(true_labels)\n",
    "\n",
    "        if cnt == 100:\n",
    "            break\n",
    "\n",
    "# 소요 시간 측정 종료\n",
    "end_time = time.time()\n",
    "\n",
    "# 소요 시간 및 정확도 출력\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# 정확도 계산\n",
    "if total_predictions > 0:\n",
    "    accuracy = (total_correct / total_predictions) * 100\n",
    "else:\n",
    "    accuracy = 0\n",
    "\n",
    "# 소요 시간 및 정확도 출력\n",
    "print(f\"소요 시간: {time_taken:.2f}초\")\n",
    "print(f\"정확도: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "많은 노력을 했지만 코드가 실행이 안 되어, 다른 참여자의 코드 실행 결과만 참고하고(코드 수정하는 부분이 없기에)\n",
    "인사이트는 제가 적겠습니다. 죄송합니다.\n",
    "\n",
    "Faster RCNN -> 소요 시간: 58.08초, 정확도: 5.28%\n",
    "\n",
    "YOLOv5 -> 소요 시간: 28.52초, 정확도: 0.28%\n",
    "\n",
    "전자는 2-stage model이고, 후자는 1-stage model이다.\n",
    "2-stage model은 소요시간이 오래 걸리는 대신 정확도가 높고,\n",
    "1-stage model은 소요시간이 짧은 대신 정확도가 떨어진다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
