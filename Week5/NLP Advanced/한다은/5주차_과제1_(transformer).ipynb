{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:\n",
        "\n",
        " self attentionì€ ëª¨ë“  ë‹¨ì–´ë“¤ì´ ì „ë¶€ í•œ ë²ˆì”© Queryê°€ ë˜ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        " attentionì˜ ê²°ê³¼ëŠ” attention value(ì‹¤ì‹œê°„ context vector), ì¦‰ Queryì˜ ì •ë³´ê°€ ë°˜ì˜ëœ key ì •ë³´ì˜ ê°€ì¤‘ì¹˜ í•©ì´ë‹¤.\n",
        "\n",
        " ëª¨ë“  ë‹¨ì–´ë“¤ì´ ì „ë¶€ í•œ ë²ˆì”© ì¿¼ë¦¬ê°€ ë˜ì–´ context embeddingì„ í•¨ìœ¼ë¡œì¨, ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê·¸ ìì‹ ì„ í¬í•¨í•œ ëª¨ë“  ë‹¨ì–´ë“¤ì˜ ì˜í–¥ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ê·¸ ë‹¨ì–´ í•˜ë‚˜ë§Œì˜ ì‚¬ì „ì  ì˜ë¯¸ê°€ ì•„ë‹Œ, ë¬¸ì¥ ë‚´ ë§¥ë½ì—ì„œ ê°€ì§€ëŠ” ì˜ë¯¸ë„ í¬í•¨í•˜ëŠ” context vectorë¥¼ ì‹œí€€ìŠ¤ë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ë‹¤."
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "fdbd59e4-83e0-4f81-ed4d-3b431844b1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê°„ë‹¨í•œ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "words = ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "word_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0, 0]),\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1, 0]),\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 0, 1])\n",
        "}\n",
        "\n",
        "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (self-attention)\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key) # ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œë‹¤.\n",
        "\n",
        "attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        ''' wordsì˜ ê° ìš”ì†Œ(í† í°)ì´ ì¿¼ë¦¬ì´ì í‚¤ë¡œ ì“°ì¸ë‹¤.\n",
        "        ì´ì¤‘ forë¬¸ì„ í†µí•´ ëª¨ë“  ë‹¨ì–´ ìŒì— ëŒ€í•´ í•œ ë²ˆì”© ë‚´ì í•˜ì—¬,\n",
        "        self attention(ìê¸° ìì‹ ì„ í¬í•¨í•œ ëª¨ë“  ë‹¨ì–´ë“¤ì˜ ì˜í–¥ì„ ë°˜ì˜)ì„ ìˆ˜í–‰í•œë‹¤.'''\n",
        "\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:\n",
        "\n",
        " Multi-Head Self Attentionì€ encoder ë‚´ ê° ë‹¨ì–´ì— ëŒ€í•´ì„œ self attentionì„ ë™ì‹œì— ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ëŠ” ê²ƒ(ì¤‘ì˜ì„± í•´ì†Œë¥¼ ìœ„í•´)ìœ¼ë¡œ, ì—¬ëŸ¬ ê°œì˜ headê°€ ë³‘ë ¬ì ìœ¼ë¡œ self attentionì„ ìˆ˜í–‰í•œë‹¤."
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—¬ëŸ¬ ê°œì˜ attention heads\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)] # head ê°œìˆ˜ëŠ” 3ê°œê°€ ê¸°ë³¸ê°’\n",
        "\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        ''' multi_head_attention_matrix í•¨ìˆ˜ì— í¬í•¨ëœ for ë¬¸ì— ì˜í•´\n",
        "        multi_head_attention_matrix[i][j]ì—ëŠ” 3ê°œì˜ ì–´í…ì…˜ ê°’ì´ ì €ì¥ëœë‹¤.\n",
        "        ì´ ê°’ì€ ië²ˆì§¸ ë‹¨ì–´ì™€ jë²ˆì§¸ ë‹¨ì–´ ê°„ì˜ attention ê°’ìœ¼ë¡œ,\n",
        "        ê° í—¤ë“œë³„ë¡œ ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ìˆë‹¤.'''\n",
        "\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "abb4c8a5-2c17-4dc2-9e43-2d7965592d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Masked Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:\n",
        "\n",
        " tì‹œì ì—ì„œ decoder ë‚´ self attentionì„ êµ¬í•  ë•Œ t+1 ì‹œì ë¶€í„°ì˜ ì •ë³´ëŠ” í™œìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤. ë””ì½”ë”ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ë²ˆì—­ ì¤‘ì¸ ìƒí™©ì´ê¸° ë•Œë¬¸ì— ë¯¸ë˜ ì‹œì ì˜ ì •ë³´ëŠ” ì°¸ê³ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤."
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë§ˆìŠ¤í¬ ì¶”ê°€: í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask\n",
        "\n",
        "mask = np.array([1, 1, 0])  # ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ëŠ” ______ (ì´ ë¹ˆì¹¸ ê¼­ ì±„ì›Œì£¼ì„¸ìš” : ë³´ì§€ ì•ŠëŠ”ë‹¤.) )\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "        '''ë‚´ì í•œ í›„(ì–´í…ì…˜ ê°’ì„ êµ¬í•œ í›„) maskì™€ ìš”ì†Œê³±ì„ ìˆ˜í–‰í•˜ì—¬ í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´(3ë²ˆì§¸)ëŠ” ë³´ì§€ ì•ŠëŠ”ë‹¤.'''\n",
        "\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fcdc9f65-e497-4e06-bf4a-38990feacbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Cross Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:\n",
        "\n",
        " queryëŠ” ë””ì½”ë” ìª½ì˜ ê²ƒ, valueì™€ keyëŠ” ì¸ì½”ë” ìª½ì˜ ê²ƒì„ í™œìš©í•˜ì—¬ attentionì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ êµì°¨í•˜ì—¬ attentionì„ ìˆ˜í–‰í•œë‹¤."
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥\n",
        "question_words = ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']\n",
        "answer_words = ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']\n",
        "question_vectors = {\n",
        "    'ë„ˆëŠ”': np.array([1, 0]),\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1])\n",
        "}\n",
        "answer_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0]),\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 1])\n",
        "}\n",
        "\n",
        "# Cross-Attention\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "        '''cross_attention_matrix[i][j]ëŠ” question_wordsì˜ ië²ˆì§¸ ë‹¨ì–´(ì¸ì½”ë”)ì™€\n",
        "        answer_wordsì˜ jë²ˆì§¸ ë‹¨ì–´(ë””ì½”ë”) ì‚¬ì´ì˜ ì–´í…ì…˜ ê°’(ë‚´ì  ê²°ê³¼)ì„ ì €ì¥í•œë‹¤.\n",
        "        ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì´ë‹¤.'''\n",
        "\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "43045e9f-e339-4fd9-db23-fbb5440aeec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}