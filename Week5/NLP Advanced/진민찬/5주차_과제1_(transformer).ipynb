{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojf8hcSd9hLC"
      },
      "source": [
        "**1. Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:\n",
        "Self-Attentionì€ ì…ë ¥ ë°ì´í„°ì˜ ê° ìš”ì†Œê°€ ë‹¤ë¥¸ ìš”ì†Œë“¤ê³¼ì˜ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ì—ì„œ Self-Attentionì€ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ë” ì˜ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ë‹¨ì–´ê°€ ë¬¸ì¥ ë‚´ì—ì„œ ì–´ëŠ ì •ë„ ì¤‘ìš”í•œì§€ë¥¼ ì¸¡ì •í•˜ê³ , ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "7c980500-9ada-4d73-e594-d91b876c0b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê°„ë‹¨í•œ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "# ê° ë‹¨ì–´ë¥¼ 3ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„í•œ í›„ 'ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤' ê°ê°ì˜ ë²¡í„°ê°’ì„ ì„¤ì •\n",
        "words = ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "word_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0, 0]),   # 'ë‚˜ëŠ”'ì„ [1, 0, 0]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1, 0]),  # 'ì‚¬ê³¼ë¥¼'ì„ [0, 1, 0]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 0, 1])   # 'ë¨¹ì—ˆë‹¤'ë¥¼ [0, 0, 1]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "}\n",
        "\n",
        "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (self-attention)\n",
        "# queryì™€ keyì˜ ë²¡í„° ê°„ ë‚´ì (dot product)ì„ ê³„ì‚°í•˜ì—¬ ë‘ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ êµ¬í•¨\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key)  # ë‘ ë²¡í„°ì˜ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "\n",
        "# Self-Attention Matrixë¥¼ ì €ì¥í•  3x3 í¬ê¸°ì˜ í–‰ë ¬ì„ ì´ˆê¸°í™”\n",
        "attention_matrix = np.zeros((len(words), len(words)))  \n",
        "\n",
        "# ëª¨ë“  ë‹¨ì–´ ìŒì— ëŒ€í•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Attention Matrixì— ì±„ì›€\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "# ê²°ê³¼ë¡œ Self-Attention Matrix ì¶œë ¥\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      },
      "source": [
        "**2. Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:Multi-Head Self-Attentionì€ Self-Attentionì˜ í™•ì¥ëœ ë²„ì „ìœ¼ë¡œ, ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ attention headsë¥¼ í†µí•´ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ë™ì‹œì— ì—¬ëŸ¬ ê´€ì ì—ì„œ íŒŒì•…í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤. ê° headëŠ” ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê³ , ì´ë¥¼ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í›„ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ë³´ë‹¤ í’ë¶€í•œ í‘œí˜„ì„ ì–»ëŠ”ë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë” ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "abb4c8a5-2c17-4dc2-9e43-2d7965592d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ],
      "source": [
        "# ì—¬ëŸ¬ ê°œì˜ attention heads\n",
        "# queryì™€ key ë²¡í„° ê°„ì˜ ë‚´ì ì„ ê° headì—ì„œ ë°˜ë³µ ìˆ˜í–‰\n",
        "# heads ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ headì˜ ê°œìˆ˜ë¥¼ ì„¤ì • (ê¸°ë³¸ê°’ì€ 3)\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # heads ìˆ˜ë§Œí¼ ë‚´ì ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "\n",
        "# 3ê°œì˜ headsë¥¼ ê°€ì§„ Multi-Head Self-Attention Matrixë¥¼ ì €ì¥í•  3D í–‰ë ¬ì„ ì´ˆê¸°í™”\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))  # (ë‹¨ì–´ ìˆ˜, ë‹¨ì–´ ìˆ˜, head ìˆ˜) í¬ê¸°ì˜ í–‰ë ¬ ìƒì„±\n",
        "\n",
        "# ëª¨ë“  ë‹¨ì–´ ìŒì— ëŒ€í•´ Multi-Head Self-Attentionì„ ìˆ˜í–‰\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "# ê²°ê³¼ë¡œ Multi-Head Self-Attention Matrix ì¶œë ¥\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHm1Y03S-hz9"
      },
      "source": [
        "**3. Masked Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Masked Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:Masked Multi-Head Self-Attentionì€ Self-Attentionì—ì„œ íŠ¹ì • ë‹¨ì–´ë¥¼ ë¬´ì‹œí•˜ê±°ë‚˜ ë³´ì§€ ì•Šë„ë¡ ë§ˆìŠ¤í¬(mask)ë¥¼ ì ìš©í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤. ì´ëŠ” ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ, ë¯¸ë˜ì˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì°¸ê³ í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–¸ì–´ ëª¨ë¸ì—ì„œëŠ” í˜„ì¬ ë‹¨ì–´ê¹Œì§€ì˜ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ, ë¯¸ë˜ì˜ ë‹¨ì–´ë“¤ì€ ê³„ì‚°ì—ì„œ ì œì™¸ëœë‹¤. ì´ë•Œ Masked Attentionì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ì˜ ë‹¨ì–´ëŠ” ë¬´ì‹œí•˜ê³  í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ë§Œì„ ì²˜ë¦¬í•˜ê²Œ ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fcdc9f65-e497-4e06-bf4a-38990feacbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# ë§ˆìŠ¤í¬ ì¶”ê°€: í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ\n",
        "# maskë¥¼ ì ìš©í•˜ì—¬ ì´í›„ ë‹¨ì–´ë“¤ì€ ë¬´ì‹œí•˜ê³  attention ê³„ì‚°\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask  # queryì™€ keyì˜ ë‚´ì ì— maskë¥¼ ê³±í•˜ì—¬ ë§ˆìŠ¤í¬ëœ ê°’ì„ ë°˜í™˜\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ì„¤ì •: ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ëŠ” ë³´ì§€ ì•ŠìŒ (ì¦‰, ì„¸ ë²ˆì§¸ ë‹¨ì–´ëŠ” ë¬´ì‹œ)\n",
        "mask = np.array([1, 1, 0])  # ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ëŠ” ë³´ì§€ ì•ŠìŒ\n",
        "\n",
        "# Masked Self-Attention Matrixë¥¼ ì €ì¥í•  í–‰ë ¬ì„ ì´ˆê¸°í™”\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "\n",
        "# ëª¨ë“  ë‹¨ì–´ ìŒì— ëŒ€í•´ Masked Self-Attentionì„ ìˆ˜í–‰\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "\n",
        "# ê²°ê³¼ë¡œ Masked Self-Attention Matrix ì¶œë ¥\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEiAlmYi-xg9"
      },
      "source": [
        "**4. Cross AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Cross Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:Cross Attentionì€ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ì‚¬ì´ì—ì„œ ìƒí˜¸ ì—°ê´€ì„±ì„ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ëŠ” \"ì§ˆë¬¸\"ê³¼ ê°™ì€ ì…ë ¥ ë¬¸ì¥ì„ ë‚˜íƒ€ë‚´ê³ , ë‹¤ë¥¸ ì‹œí€€ìŠ¤ëŠ” \"ì‘ë‹µ\" ë˜ëŠ” ì¶œë ¥ ë¬¸ì¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ë ¨ì„±ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥ì—ì„œ ê° ìš”ì†Œê°€ ì…ë ¥ì˜ ì–´ëŠ ë¶€ë¶„ì— ì£¼ëª©í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ì´ëŠ” íŠ¹íˆ ë²ˆì—­ ì‹œìŠ¤í…œì´ë‚˜ ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œê³¼ ê°™ì€ ì‘ì—…ì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "43045e9f-e339-4fd9-db23-fbb5440aeec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥\n",
        "# ì§ˆë¬¸ ë¬¸ì¥: ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']\n",
        "# ì‘ë‹µ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']\n",
        "question_words = ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']\n",
        "answer_words = ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']\n",
        "\n",
        "# ì§ˆë¬¸ ë¬¸ì¥ì— ëŒ€í•œ ë²¡í„° ì •ì˜\n",
        "question_vectors = {\n",
        "    'ë„ˆëŠ”': np.array([1, 0]),   # 'ë„ˆëŠ”'ì„ [1, 0]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1])  # 'ì‚¬ê³¼ë¥¼'ì„ [0, 1]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "}\n",
        "\n",
        "# ì‘ë‹µ ë¬¸ì¥ì— ëŒ€í•œ ë²¡í„° ì •ì˜\n",
        "answer_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0]),   # 'ë‚˜ëŠ”'ì„ [1, 0]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 1])  # 'ë¨¹ì—ˆë‹¤'ë¥¼ [0, 1]ìœ¼ë¡œ ë²¡í„°í™”\n",
        "}\n",
        "\n",
        "# Cross-Attention Matrix ì´ˆê¸°í™” (ì§ˆë¬¸ ë‹¨ì–´ x ì‘ë‹µ ë‹¨ì–´ í¬ê¸°)\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "\n",
        "# ëª¨ë“  ì§ˆë¬¸ ë‹¨ì–´ì™€ ì‘ë‹µ ë‹¨ì–´ ìŒì— ëŒ€í•´ ìœ ì‚¬ë„(ë‚´ì )ë¥¼ ê³„ì‚°\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        # ì§ˆë¬¸ ë‹¨ì–´ì™€ ì‘ë‹µ ë‹¨ì–´ì˜ ë²¡í„° ê°„ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ì €ì¥\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "\n",
        "# ê²°ê³¼ë¡œ Cross-Attention Matrix ì¶œë ¥\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
