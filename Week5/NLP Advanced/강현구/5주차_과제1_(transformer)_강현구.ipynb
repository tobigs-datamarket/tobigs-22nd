{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Self Attention📌**\n","\n","아래 코드를 수행해보고, Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 문장 내 단어들끼리의 연관성을 보고 단어끼리의 연관성이 높은 단어에 큰 가중치(attention score)를 부여하여 강조하는 알고리즘\n"],"metadata":{"id":"ojf8hcSd9hLC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2KyObPG9Z7v","outputId":"7c980500-9ada-4d73-e594-d91b876c0b2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n"]}],"source":["import numpy as np\n","\n","# 간단한 문장: ['나는', '사과를', '먹었다']\n","words = ['나는', '사과를', '먹었다']\n","word_vectors = {\n","    '나는': np.array([1, 0, 0]),\n","    '사과를': np.array([0, 1, 0]),\n","    '먹었다': np.array([0, 0, 1])\n","}\n","\n","# 유사도 계산 함수 (self-attention)\n","def self_attention(query, key):\n","    return np.dot(query, key)\n","\n","# 유사도 행렬 계산\n","attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n","\n","print(\"Self-Attention Matrix:\")\n","print(attention_matrix)\n"]},{"cell_type":"markdown","source":["**2. Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: msa는 sa를 병렬적으로 여러번 수행하는 것입니다"],"metadata":{"id":"WA3NEBQC-Dpg"}},{"cell_type":"code","source":["# 여러 개의 attention heads\n","def multi_head_self_attention(query, key, heads=3):\n","    return [np.dot(query, key) for _ in range(heads)]\n","\n","multi_head_attention_matrix = np.zeros((len(words), len(words), 3))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n","\n","print(\"\\nMulti-Head Self-Attention Matrix:\")\n","print(multi_head_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1bvP6lC-efR","outputId":"abb4c8a5-2c17-4dc2-9e43-2d7965592d86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Multi-Head Self-Attention Matrix:\n","[[[1. 1. 1.]\n","  [0. 0. 0.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [1. 1. 1.]\n","  [0. 0. 0.]]\n","\n"," [[0. 0. 0.]\n","  [0. 0. 0.]\n","  [1. 1. 1.]]]\n"]}]},{"cell_type":"markdown","source":["**3. Masked Multi-Head Self Attention📌**\n","\n","아래 코드를 수행해보고, Masked Multi-Head Self Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 다음 단어를 예측할 때 미래 단어들은 마스킹하여 보지 않습니다"],"metadata":{"id":"lHm1Y03S-hz9"}},{"cell_type":"code","source":["# 마스크 추가: 현재 단어 이후의 단어는 계산하지 않음\n","def masked_attention(query, key, mask):\n","    return np.dot(query, key) * mask\n","\n","mask = np.array([1, 1, 0])  # 첫 번째, 두 번째는 보지만, 세 번째는 보지 않습니다 (이 빈칸 꼭 채워주세요 :) )\n","masked_attention_matrix = np.zeros((len(words), len(words)))\n","for i in range(len(words)):\n","    for j in range(len(words)):\n","        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n","\n","print(\"\\nMasked Self-Attention Matrix:\")\n","print(masked_attention_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hELE_Nyf-pOP","outputId":"fcdc9f65-e497-4e06-bf4a-38990feacbde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Masked Self-Attention Matrix:\n","[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["**4. Cross Attention📌**\n","\n","아래 코드를 수행해보고, Cross Attention은 어떤 메커니즘인지 설명하고,\n","\n"," 그 설명에 맞게 각 코드에 직접 주석을 달아봅시다.\n","\n"," ✅설명: 현재 문장과 다른 문장의 단어들끼리의 연관성을 보고 높은 단어에 socre를 부여합니다"],"metadata":{"id":"wEiAlmYi-xg9"}},{"cell_type":"code","source":["# 입력 문장과 응답 문장\n","question_words = ['너는', '사과를']\n","answer_words = ['나는', '먹었다']\n","question_vectors = {\n","    '너는': np.array([1, 0]),\n","    '사과를': np.array([0, 1])\n","}\n","answer_vectors = {\n","    '나는': np.array([1, 0]),\n","    '먹었다': np.array([0, 1])\n","}\n","\n","# Cross-Attention\n","cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n","for i in range(len(question_words)):\n","    for j in range(len(answer_words)):\n","        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n","\n","print(\"\\nCross-Attention Matrix:\")\n","print(cross_attention_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2871Q9r-5ZP","outputId":"43045e9f-e339-4fd9-db23-fbb5440aeec4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cross-Attention Matrix:\n","[[1. 0.]\n"," [0. 1.]]\n"]}]}]}