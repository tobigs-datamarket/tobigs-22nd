{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:   Attentionì¤‘ì—ì„œ ìê¸° ìì‹ ì—ê²Œ Attention ë©”ì»¤ë‹ˆì¦˜ì„ í–‰í•˜ëŠ” ë°©ì‹. (Query = Key = Value)"
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "ee978a47-c465-4c6a-a7ea-3fd0b8f845a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê°„ë‹¨í•œ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "words = ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "\n",
        "# word ë¥¼ vector ë¡œ embedding\n",
        "word_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0, 0]),\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1, 0]),\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 0, 1])\n",
        "}\n",
        "\n",
        "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (self-attention)\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key) # query ì™€ key ë‚´ì  (ë‘˜ ì‚¬ì´ ì—°ê´€ì„± ê³„ì‚° ìœ„í•´ì„œ)\n",
        "\n",
        "attention_matrix = np.zeros((len(words), len(words))) # ìœ„ì˜ ë‚´ì  ê°’ì´ attention score ë¡œ ì €ì¥í•˜ê¸° ìœ„í•´ ë™ì¼ í¬ê¸°ì˜ 0 ë²¡í„° ìƒì„±\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]]) # ê° ë²¡í„°ê°„ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ë‚´ì  ê³„ì‚°(ìê¸°ìì‹ ê³¼ ê³„ì‚° ê²½ìš°ë„ ìˆìŒ)\n",
        "\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…: Query, Key, Valueê°’ì„ í•œ ë²ˆì— ê³„ì‚°í•˜ì§€ ì•Šê³  head ìˆ˜ë§Œí¼ ë‚˜ëˆ  ê³„ì‚° í›„ ë‚˜ì¤‘ì— Attention Valueë“¤ì„ í•©ì¹˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. í•œë§ˆë””ë¡œ ë¶„í•  ê³„ì‚° í›„ í•©ì‚°í•˜ëŠ” ë°©ì‹.\n",
        "\n",
        "Multi-head Attention\n",
        "\n",
        "1. ì›ë˜ Query, Key, Value í–‰ë ¬ ê°’ì„ head ìˆ˜ë§Œí¼ ë¶„í• \n",
        "\n",
        "2. ë¶„í• ëœ í–‰ë ¬ ê°’ì„ í†µí•´, ê° Attention valueê°’ë“¤ì„ ë„ì¶œ\n",
        "\n",
        "3. ë„ì¶œëœ Attention valueê°’ë“¤ì„ concatenate(ìŒ“ì•„ í•©ì¹˜ê¸°)í•˜ì—¬ ìµœì¢… Attention valueë„ì¶œ"
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—¬ëŸ¬ ê°œì˜ attention heads\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # ê° í—¤ë“œì— ëŒ€í•´ì„œ attention ê³„ì‚°\n",
        "\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3)) # head 3ê°œ ì´ë¯€ë¡œ attention value ì €ì¥í•  0 ë§¤íŠ¸ë¦¬ìŠ¤ ìƒì„±\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "dde5b15d-f4ff-4b26-dfe7-7096577380f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Masked Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…: ë””ì½”ë”(Decoder)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, ë‹¤ì¤‘ í—¤ë“œ(Self-Attention)ì˜ ë³€í˜•ì…ë‹ˆë‹¤. ì£¼ìš” ëª©í‘œëŠ” ë¯¸ë˜ ì •ë³´ë¥¼ ì°¨ë‹¨í•˜ì—¬, ëª¨ë¸ì´ ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ í˜„ì¬ ì‹œì  ì´í›„ì˜ ë‹¨ì–´ë“¤ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ê²ƒì´ê³ , ë©”ì»¤ë‹ˆì¦˜ì€\n",
        "1. Query, Key, Value ë²¡í„° ìƒì„± í›„,\n",
        "\n",
        "2. ë¯¸ë˜ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ë§ˆìŠ¤í‚¹ ì§„í–‰ë˜ëŠ” ë° ë§ˆìŠ¤í‚¹ í–‰ë ¬ì€ ìƒì‚¼ê° í–‰ë ¬(Upper Triangular Matrix) í˜•íƒœë¡œ ì´ë ‡ê²Œ í•˜ë©´ ê° ë‹¨ì–´ê°€ ìì‹ ì„ í¬í•¨í•´ ê·¸ ì´ì „ ë‹¨ì–´ë“¤ì—ë§Œ Attentionì„ í•  ìˆ˜ ìˆê³ , ì´í›„ì˜ ë‹¨ì–´ëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.\n",
        "3. Attention Score ê³„ì‚° ë° ë§ˆìŠ¤í‚¹ ì ìš©: ë§ˆìŠ¤í¬ í–‰ë ¬ì— 0 ëŒ€ì‹  ë§¤ìš° í° ìŒìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì— ì˜í•´ í•´ë‹¹ ê°€ì¤‘ì¹˜ê°€ 0ìœ¼ë¡œ ë˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤. ì¦‰, ë¯¸ë˜ ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ëŠ” 0ì´ ë˜ê³ , ëª¨ë¸ì€ ì˜¤ì§ ìì‹ ê³¼ ê·¸ ì´ì „ ë‹¨ì–´ë“¤ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë§ˆìŠ¤í¬ ì¶”ê°€: í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask #\n",
        "\n",
        "mask = np.array([1, 1, 0])  # ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ëŠ” Mask (ì´ ë¹ˆì¹¸ ê¼­ ì±„ì›Œì£¼ì„¸ìš” :) )\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
        "for i in range(len(words)):\n",
        "    for j in range(len(words)):\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "fabed0f9-215a-4f7c-80a0-910c69d4b1e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Cross Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…: ë””ì½”ë”ì˜ Query ë²¡í„°ê°€ ì¸ì½”ë”ì˜ Key ë° Value ë²¡í„°ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì¦‰, ë””ì½”ë”ëŠ” ìê¸° ìì‹ ì— ëŒ€í•œ ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ ì¸ì½”ë”ê°€ ìƒì„±í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥\n",
        "question_words = ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']\n",
        "answer_words = ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']\n",
        "question_vectors = {\n",
        "    'ë„ˆëŠ”': np.array([1, 0]),\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1])\n",
        "}\n",
        "answer_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0]),\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 1])\n",
        "}\n",
        "\n",
        "# Cross-Attention\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "for i in range(len(question_words)):\n",
        "    for j in range(len(answer_words)):\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]]) # question ê³¼ answer ì´ ìƒí˜¸ì‘ìš©\n",
        "\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "325639d3-f019-4126-a75a-b70f4c8ef5d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTpMpFQpCFxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}