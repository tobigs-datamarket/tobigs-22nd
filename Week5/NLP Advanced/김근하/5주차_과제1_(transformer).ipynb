{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**1. Self AttentionğŸ“Œ**\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
    "\n",
    " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
    "\n",
    " âœ…ì„¤ëª…: í•˜ë‚˜ì˜ ë²¡í„° ë‚´ì—ì„œ ê° í† í°ê°„ì˜ Weightsë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •ì´ë‹¤."
   ],
   "metadata": {
    "id": "ojf8hcSd9hLC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2KyObPG9Z7v",
    "outputId": "7c980500-9ada-4d73-e594-d91b876c0b2d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Self-Attention Matrix:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ê°„ë‹¨í•œ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
    "words = ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤'] #vector\n",
    "word_vectors = {\n",
    "    'ë‚˜ëŠ”': np.array([1, 0, 0]),\n",
    "    'ì‚¬ê³¼ë¥¼': np.array([0, 1, 0]),\n",
    "    'ë¨¹ì—ˆë‹¤': np.array([0, 0, 1])\n",
    "}\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (self-attention)\n",
    "def self_attention(query, key): \n",
    "    return np.dot(query, key)\n",
    "\n",
    "attention_matrix = np.zeros((len(words), len(words)))\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])#self attention with dot product\n",
    "\n",
    "print(\"Self-Attention Matrix:\")\n",
    "print(attention_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Multi-Head Self AttentionğŸ“Œ**\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
    "\n",
    " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
    "\n",
    " âœ…ì„¤ëª…: ê°™ì€ ë²¡í„°ë‚´ì—ì„œ ì—¬ëŸ¬ë²ˆì˜ self attentionì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì´ë‹¤. self attentionê³¼ ë‹¤ë¥´ê²Œ ë‹¤ì–‘í•œ ê´€ì ì˜ self attentionì´ ê°€ëŠ¥í•˜ë‹¤."
   ],
   "metadata": {
    "id": "WA3NEBQC-Dpg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ì—¬ëŸ¬ ê°œì˜ attention heads\n",
    "def multi_head_self_attention(query, key, heads=3):\n",
    "    return [np.dot(query, key) for _ in range(heads)]\n",
    "\n",
    "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]]) #multi head attention with dot product\n",
    "\n",
    "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
    "print(multi_head_attention_matrix)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1bvP6lC-efR",
    "outputId": "abb4c8a5-2c17-4dc2-9e43-2d7965592d86"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Multi-Head Self-Attention Matrix:\n",
      "[[[1. 1. 1.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [1. 1. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 1. 1.]]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Masked Multi-Head Self AttentionğŸ“Œ**\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Masked Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
    "\n",
    " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
    "\n",
    " âœ…ì„¤ëª…: ì¼ë¶€ í† í°ì„ mask ì²˜ë¦¬í•˜ì—¬ ëª¨ë¸ì´ mask ë˜ì–´ ìˆëŠ” ê³³ì— ëŒ€í•œ ì •ë³´ë¥¼ ìœ ì¶”í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ëª©í‘œê°€ ìˆë‹¤."
   ],
   "metadata": {
    "id": "lHm1Y03S-hz9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ë§ˆìŠ¤í¬ ì¶”ê°€: í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ\n",
    "def masked_attention(query, key, mask):\n",
    "    return np.dot(query, key) * mask\n",
    "\n",
    "mask = np.array([1, 1, 0])  # ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ëŠ” ______ (ì´ ë¹ˆì¹¸ ê¼­ ì±„ì›Œì£¼ì„¸ìš” :) )\n",
    "masked_attention_matrix = np.zeros((len(words), len(words)))\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
    "#masked attention with dot product\n",
    "print(\"\\nMasked Self-Attention Matrix:\")\n",
    "print(masked_attention_matrix)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hELE_Nyf-pOP",
    "outputId": "fcdc9f65-e497-4e06-bf4a-38990feacbde"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Masked Self-Attention Matrix:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Cross AttentionğŸ“Œ**\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Cross Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
    "\n",
    " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
    "\n",
    " âœ…ì„¤ëª…: ì„œë¡œ ë‹¤ë¥¸ ë²¡í„°ê°€ ìƒí˜¸ ì •ë³´ë¥¼ attention í•˜ëŠ” ê³¼ì •ì´ë‹¤."
   ],
   "metadata": {
    "id": "wEiAlmYi-xg9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥\n",
    "question_words = ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']\n",
    "answer_words = ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']\n",
    "question_vectors = {\n",
    "    'ë„ˆëŠ”': np.array([1, 0]),\n",
    "    'ì‚¬ê³¼ë¥¼': np.array([0, 1])\n",
    "}\n",
    "answer_vectors = {\n",
    "    'ë‚˜ëŠ”': np.array([1, 0]),\n",
    "    'ë¨¹ì—ˆë‹¤': np.array([0, 1])\n",
    "}\n",
    "\n",
    "# Cross-Attention\n",
    "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
    "for i in range(len(question_words)):\n",
    "    for j in range(len(answer_words)):\n",
    "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]]) #Cross attention with dot product\n",
    "\n",
    "print(\"\\nCross-Attention Matrix:\")\n",
    "print(cross_attention_matrix)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2871Q9r-5ZP",
    "outputId": "43045e9f-e339-4fd9-db23-fbb5440aeec4"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Cross-Attention Matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ]
  }
 ]
}
