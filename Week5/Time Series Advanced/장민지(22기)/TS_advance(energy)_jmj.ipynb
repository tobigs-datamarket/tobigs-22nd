{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7M6mqavCdgs"
      },
      "source": [
        "## Q. 기상 실측 데이터를 토대로 태양광 발전량인 amount(2023-10-15) 값을 예측해보시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UunFQBs9FaX0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "pd.set_option('display.max_column', 20)\n",
        "pd.set_option('display.max_row', 100)\n",
        "\n",
        "data = pd.read_csv('./energy_tobigs_question.csv')\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "\n",
        "# 피처와 타겟 변수 설정\n",
        "features = ['cloud', 'temp', 'humidity', 'ground_press', 'wind_speed', 'wind_dir', 'rain', 'snow', 'dew_point', 'vis', 'uv_idx', 'azimuth', 'elevation']\n",
        "target = 'amount'\n",
        "\n",
        "# 데이터셋 분리 (2023-10-14까지 학습, 2023-10-15 하루 예측)\n",
        "train_data = data[data['time'] < '2023-10-15'].copy()\n",
        "test_data = data[(data['time'] >= '2023-10-15') & (data['time'] < '2023-10-16')].copy()\n",
        "\n",
        "# 학습 데이터에 NaN 값 제거\n",
        "train_data = train_data.dropna()\n",
        "\n",
        "# 데이터 스케일링\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train_data.loc[:, features] = scaler_x.fit_transform(train_data[features])\n",
        "train_data.loc[:, target] = scaler_y.fit_transform(train_data[[target]])\n",
        "\n",
        "test_data.loc[:, features] = scaler_x.transform(test_data[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dne_kc6_Fn6Z"
      },
      "source": [
        "### 시퀀스 데이터 생성 함수 작성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkdLvF-nFrxQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 시퀀스 데이터 생성 함수\n",
        "# dataX: 시퀀스 데이터를 담은 리스트 (예: [[시퀀스1], [시퀀스2], ...])\n",
        "# dataY: 각 시퀀스에 대응하는 타겟 값 (예: 발전량)\n",
        "\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(time_series) - seq_length):\n",
        "        dataX.append(time_series[i:i+seq_length, :-1])\n",
        "        dataY.append(time_series[i+seq_length, [-1]])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "seq_length = 24  # (1) 예: 24시간의 데이터를 시퀀스로 사용\n",
        "batch_size = 32  # (2) 배치 크기 설정\n",
        "\n",
        "# 학습 데이터 생성\n",
        "# trainX: 24시간의 피처 데이터\n",
        "# trainY: 그 다음 시간에 대한 발전량 값\n",
        "trainX, trainY = build_dataset(time_series_df.values, seq_length)  # (3) 데이터프레임의 values를 사용\n",
        "\n",
        "# 텐서로 변환\n",
        "trainX_tensor = torch.tensor(trainX, dtype=torch.float32)  # (4) numpy 배열을 텐서로 변환\n",
        "trainY_tensor = torch.tensor(trainY, dtype=torch.float32)  # (5) numpy 배열을 텐서로 변환\n",
        "\n",
        "# 텐서데이터셋 생성\n",
        "train_dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # (6) 배치 크기 설정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZtaJNcVFsLJ"
      },
      "source": [
        "### 학습한 시계열 모델을 사용해서 예측을 해보세요 (RNN, LSTM, Transformer, Informer 중에서 선택적으로 활용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUEsSYdsGAaY"
      },
      "outputs": [],
      "source": [
        "# 슬라이딩 윈도우 방식으로 2023-10-15의 24시간 발전량 예측해보기 (Direct Multi-step Forecast Strategy 혹은 Recursive Multi-step Forecast 등 이외의 방법론도 자유롭게 사용 가능)\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/C:\\Users\\장민지\\Downloads/energy_tobigs_question.csv' \n",
        "energy_data = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocess the data\n",
        "energy_data['time'] = pd.to_datetime(energy_data['time'])\n",
        "energy_data = energy_data.set_index('time')\n",
        "energy_data = energy_data.asfreq('H')\n",
        "energy_data = energy_data.interpolate(method='linear')\n",
        "\n",
        "# Define feature columns and target\n",
        "features = ['cloud', 'temp', 'humidity', 'ground_press', 'wind_speed', 'wind_dir', \n",
        "            'rain', 'snow', 'dew_point', 'vis', 'uv_idx', 'azimuth', 'elevation']\n",
        "target = 'amount'\n",
        "\n",
        "# Extract training data up to '2023-10-14 23:00:00'\n",
        "training_data = energy_data.loc[:'2023-10-14 23:00:00']\n",
        "\n",
        "# Sliding window dataset preparation\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(time_series) - seq_length):\n",
        "        dataX.append(time_series[i:i + seq_length, :-1])\n",
        "        dataY.append(time_series[i + seq_length, -1])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "seq_length = 24  # 24 hours sequence\n",
        "trainX, trainY = build_dataset(training_data[features + [target]].values, seq_length)\n",
        "\n",
        "# Convert to tensors\n",
        "trainX_tensor = torch.tensor(trainX, dtype=torch.float32)\n",
        "trainY_tensor = torch.tensor(trainY, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# LSTM Model Definition\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "input_size = len(features)\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_X, batch_Y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs.squeeze(), batch_Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Predict for 2023-10-15 using the trained model\n",
        "model.eval()\n",
        "last_24_hours_data = energy_data.loc['2023-10-14 00:00:00':'2023-10-14 23:00:00'][features].values\n",
        "predictions = []\n",
        "\n",
        "# Recursive prediction for the next 24 hours\n",
        "current_input = torch.tensor(last_24_hours_data, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "for _ in range(24):\n",
        "    with torch.no_grad():\n",
        "        next_pred = model(current_input).item()\n",
        "        predictions.append(next_pred)\n",
        "        \n",
        "        # Update the current input with the latest prediction\n",
        "        next_input = np.append(current_input.squeeze(0).numpy(), np.array([[next_pred] * len(features)]), axis=0)\n",
        "        current_input = torch.tensor(next_input[-seq_length:, :], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "# Display predictions for 2023-10-15\n",
        "for hour, pred in enumerate(predictions, start=1):\n",
        "    print(f\"2023-10-15 {hour:02}:00: Predicted Generation: {pred:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXnKhcF7Gd22"
      },
      "source": [
        "### 2023-10-15 발전량 예측값을 predicted_amounts에 저장하고 시각화를 실행해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85a5LYliGbci"
      },
      "outputs": [],
      "source": [
        "# 시각화\n",
        "fig = go.Figure()\n",
        "\n",
        "# 실제 발전량 시각화\n",
        "fig.add_trace(go.Scatter(x=data['time'], y=data['amount'],\n",
        "                         mode='lines', name='Actual Amount'))\n",
        "\n",
        "# 예측된 발전량 시각화 (2023-10-15의 24시간 동안의 예측)\n",
        "fig.add_trace(go.Scatter(x=test_data['time'], y=predicted_amounts,\n",
        "                         mode='lines', name='Predicted Amount', line=dict(dash='dot', color='red')))\n",
        "\n",
        "# 레이아웃 설정\n",
        "fig.update_layout(title='2023-10-15 24시간 발전량 예측',\n",
        "                  xaxis_title='시간',\n",
        "                  yaxis_title='발전량 (kWh)')\n",
        "\n",
        "# 그래프 출력\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qc4G_sIPYW"
      },
      "source": [
        "### 정답을 적어주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxu_nPKAIRiq"
      },
      "outputs": [],
      "source": [
        "# 예측된 24시간 발전량 출력\n",
        "print(f'2023-10-15 예측 발전량 (24시간): {predicted_amounts} kWh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngKvGGwZIFLY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG2WPZYKEOvn"
      },
      "source": [
        "### (필수) Informer모델은 transformer 모델의 어떤 부분을 개선하고자 했나요? (차이점을 중심으로 서술)\n",
        "Transformer는 모든 입력 토큰 간의 관계를 계산하는 완전한 self-attention 메커니즘을 사용하기 때문에 시계열 데이터처럼 길이가 긴 데이터에 비효율적입니다.\n",
        "Informer는 이러한 비효율성을 해결하기 위해 ProbSparse Self-Attention을 도입했습니다. 이를 통해 모든 쿼리-키 조합을 계산하는 대신, 정보량이 풍부한 쿼리-키 조합만을 선택하여 계산 복잡도를 크게 줄일 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxrmj2bGGpKx"
      },
      "source": [
        "### (필수) 모델링 해석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 모델을 선택했다면 왜 선택했는지 본인만의 근거를 정리해주세요.\n",
        "# 더 나아가 파라미터 선택의 기준이 있었다면 좋습니다.\n",
        "\n",
        "LSTM\n",
        "1. 시계열 데이터의 특성: 발전량 데이터는 시간에 따라 변화하는 시계열 데이터이며, 과거의 상태가 미래의 상태에 영향을 미치는 연속성이 있습니다. LSTM(Long Short-Term Memory)은 시계열 데이터에서 장기 의존성을 효과적으로 학습할 수 있는 모델이기 때문에 적합합니다.\n",
        "  \n",
        "2. 문맥 정보 유지: 일반적인 순환신경망(RNN)은 시계열이 길어지면 과거 정보가 손실될 수 있지만, LSTM은 게이트 구조(입력 게이트, 망각 게이트, 출력 게이트)를 통해 이전 시점의 중요한 정보를 유지하거나 버릴 수 있습니다. 이를 통해 발전량 예측에 필요한 과거 패턴을 잘 학습할 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
