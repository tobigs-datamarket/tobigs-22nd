{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRlb757-wpKb"
      },
      "source": [
        "#1-1\n",
        "\n",
        "MNIST 데이터셋을 사용하여 간단한 GAN을 구현한 코드입니다.\n",
        "\n",
        "코드를 실행시키고, 주석을 달아주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2a96QRuEwV-c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_D5vGGkTwXFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a1fbd1-c8e2-491a-f659-bbc4e64e1e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4585226.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134182.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1085383.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4363382.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "learning_rate = 0.0002\n",
        "img_size = 28 * 28\n",
        "noise_size = 100\n",
        "hidden_size1 = 256\n",
        "hidden_size2 = 512\n",
        "hidden_size3 = 1024\n",
        "dir_name = \"GAN_results\"\n",
        "\n",
        "# GPU 사용 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 결과 이미지를 저장할 디렉토리 생성\n",
        "if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)\n",
        "\n",
        "# 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "MNIST_dataset = datasets.MNIST(root='../../data/',\n",
        "                               train=True,\n",
        "                               transform=transform,\n",
        "                               download=True)\n",
        "\n",
        "# DataLoader 설정\n",
        "data_loader = torch.utils.data.DataLoader(dataset=MNIST_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BzKto3wuv_Mk"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(img_size, hidden_size3), # 입력 이미지 크기를 hidden_size3으로 변환\n",
        "            nn.LeakyReLU(0.2), # Leaky ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size3, hidden_size2), # hidden_size3을 hidden_size2로 변환\n",
        "            nn.LeakyReLU(0.2), # Leaky ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size2, hidden_size1), # hidden_size2를 hidden_size1로 변환\n",
        "            nn.LeakyReLU(0.2), # Leaky ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size1, 1), # 최종 출력을 위한 선형 변환\n",
        "            nn.Sigmoid() # Sigmoid 함수를 통해 0과 1 사이의 확률 값으로 변환\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 x를 모델에 통과시켜 결과를 반환\n",
        "        return self.model(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_size, hidden_size1), # 입력 노이즈를 hidden_size1로 변환\n",
        "            nn.ReLU(), # ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size1, hidden_size2), # hidden_size1을 hidden_size2로 변환\n",
        "            nn.ReLU(), # ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size2, hidden_size3), # hidden_size2를 hidden_size3으로 변환\n",
        "            nn.ReLU(), # ReLU 활성화 함수\n",
        "            nn.Linear(hidden_size3, img_size), # hidden_size3을 이미지 크기로 변환\n",
        "            nn.Tanh() # Tanh 함수를 통해 출력 범위를 -1과 1로 조정\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 x를 모델에 통과시켜 결과를 반환\n",
        "        return self.model(x)\n",
        "\n",
        "# Discriminator와 Generator 모델 정의 및 GPU로 이동\n",
        "discriminator = Discriminator().to(device)\n",
        "generator = Generator().to(device)\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VwZVMuXawdQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbc4daf-ed28-4c3a-d128-646db55fef5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [150/600], D Loss: 0.0341, G Loss: 3.5758\n",
            "Epoch [1/100], Step [300/600], D Loss: 0.2146, G Loss: 17.0593\n",
            "Epoch [1/100], Step [450/600], D Loss: 0.0210, G Loss: 6.6702\n",
            "Epoch [1/100], Step [600/600], D Loss: 0.0917, G Loss: 12.5119\n",
            "---------Epoch [1/100] : D Performance: 0.97, G Performance: 0.00\n",
            "Epoch [2/100], Step [150/600], D Loss: 0.0039, G Loss: 7.0128\n",
            "Epoch [2/100], Step [300/600], D Loss: 0.1460, G Loss: 13.1850\n",
            "Epoch [2/100], Step [450/600], D Loss: 0.2478, G Loss: 7.3226\n",
            "Epoch [2/100], Step [600/600], D Loss: 0.1174, G Loss: 9.8547\n",
            "---------Epoch [2/100] : D Performance: 0.90, G Performance: 0.01\n",
            "Epoch [3/100], Step [150/600], D Loss: 0.3117, G Loss: 4.1842\n",
            "Epoch [3/100], Step [300/600], D Loss: 0.3027, G Loss: 8.1486\n",
            "Epoch [3/100], Step [450/600], D Loss: 0.2091, G Loss: 3.2236\n",
            "Epoch [3/100], Step [600/600], D Loss: 0.8571, G Loss: 2.3695\n",
            "---------Epoch [3/100] : D Performance: 0.55, G Performance: 0.35\n",
            "Epoch [4/100], Step [150/600], D Loss: 0.3877, G Loss: 1.2452\n",
            "Epoch [4/100], Step [300/600], D Loss: 0.4082, G Loss: 1.0310\n",
            "Epoch [4/100], Step [450/600], D Loss: 0.4833, G Loss: 4.5940\n",
            "Epoch [4/100], Step [600/600], D Loss: 0.0164, G Loss: 9.1589\n",
            "---------Epoch [4/100] : D Performance: 0.99, G Performance: 0.01\n",
            "Epoch [5/100], Step [150/600], D Loss: 0.7596, G Loss: 0.7704\n",
            "Epoch [5/100], Step [300/600], D Loss: 0.7517, G Loss: 0.9210\n",
            "Epoch [5/100], Step [450/600], D Loss: 0.2212, G Loss: 1.9360\n",
            "Epoch [5/100], Step [600/600], D Loss: 0.2124, G Loss: 3.0685\n",
            "---------Epoch [5/100] : D Performance: 0.81, G Performance: 0.08\n",
            "Epoch [6/100], Step [150/600], D Loss: 0.3430, G Loss: 2.5614\n",
            "Epoch [6/100], Step [300/600], D Loss: 0.0749, G Loss: 3.1575\n",
            "Epoch [6/100], Step [450/600], D Loss: 0.2052, G Loss: 2.1871\n",
            "Epoch [6/100], Step [600/600], D Loss: 0.2083, G Loss: 2.0805\n",
            "---------Epoch [6/100] : D Performance: 0.90, G Performance: 0.10\n",
            "Epoch [7/100], Step [150/600], D Loss: 0.1002, G Loss: 3.3247\n",
            "Epoch [7/100], Step [300/600], D Loss: 0.2019, G Loss: 2.6308\n",
            "Epoch [7/100], Step [450/600], D Loss: 0.1030, G Loss: 3.8467\n",
            "Epoch [7/100], Step [600/600], D Loss: 0.2697, G Loss: 3.9660\n",
            "---------Epoch [7/100] : D Performance: 0.87, G Performance: 0.09\n",
            "Epoch [8/100], Step [150/600], D Loss: 0.2689, G Loss: 3.2112\n",
            "Epoch [8/100], Step [300/600], D Loss: 0.1262, G Loss: 4.2682\n",
            "Epoch [8/100], Step [450/600], D Loss: 0.1249, G Loss: 4.3564\n",
            "Epoch [8/100], Step [600/600], D Loss: 0.1908, G Loss: 6.5160\n",
            "---------Epoch [8/100] : D Performance: 0.94, G Performance: 0.02\n",
            "Epoch [9/100], Step [150/600], D Loss: 0.0422, G Loss: 5.5848\n",
            "Epoch [9/100], Step [300/600], D Loss: 0.0847, G Loss: 4.2609\n",
            "Epoch [9/100], Step [450/600], D Loss: 0.2376, G Loss: 5.4058\n",
            "Epoch [9/100], Step [600/600], D Loss: 0.2390, G Loss: 3.3909\n",
            "---------Epoch [9/100] : D Performance: 0.87, G Performance: 0.05\n",
            "Epoch [10/100], Step [150/600], D Loss: 0.2849, G Loss: 3.3332\n",
            "Epoch [10/100], Step [300/600], D Loss: 0.1311, G Loss: 3.2666\n",
            "Epoch [10/100], Step [450/600], D Loss: 0.1232, G Loss: 3.9024\n",
            "Epoch [10/100], Step [600/600], D Loss: 0.0751, G Loss: 7.5140\n",
            "---------Epoch [10/100] : D Performance: 0.94, G Performance: 0.00\n",
            "Epoch [11/100], Step [150/600], D Loss: 0.1404, G Loss: 4.7671\n",
            "Epoch [11/100], Step [300/600], D Loss: 0.0934, G Loss: 3.7061\n",
            "Epoch [11/100], Step [450/600], D Loss: 0.0512, G Loss: 5.3346\n",
            "Epoch [11/100], Step [600/600], D Loss: 0.1253, G Loss: 4.1065\n",
            "---------Epoch [11/100] : D Performance: 0.93, G Performance: 0.02\n",
            "Epoch [12/100], Step [150/600], D Loss: 0.1328, G Loss: 5.8906\n",
            "Epoch [12/100], Step [300/600], D Loss: 0.1069, G Loss: 6.6192\n",
            "Epoch [12/100], Step [450/600], D Loss: 0.1208, G Loss: 5.0127\n",
            "Epoch [12/100], Step [600/600], D Loss: 0.0616, G Loss: 7.3910\n",
            "---------Epoch [12/100] : D Performance: 0.97, G Performance: 0.01\n",
            "Epoch [13/100], Step [150/600], D Loss: 0.0917, G Loss: 4.7284\n",
            "Epoch [13/100], Step [300/600], D Loss: 0.4908, G Loss: 4.1066\n",
            "Epoch [13/100], Step [450/600], D Loss: 0.1341, G Loss: 4.3264\n",
            "Epoch [13/100], Step [600/600], D Loss: 0.1495, G Loss: 5.2075\n",
            "---------Epoch [13/100] : D Performance: 0.90, G Performance: 0.01\n",
            "Epoch [14/100], Step [150/600], D Loss: 0.4047, G Loss: 7.0044\n",
            "Epoch [14/100], Step [300/600], D Loss: 0.1340, G Loss: 5.2718\n",
            "Epoch [14/100], Step [450/600], D Loss: 0.1094, G Loss: 4.4530\n",
            "Epoch [14/100], Step [600/600], D Loss: 0.1009, G Loss: 3.6906\n",
            "---------Epoch [14/100] : D Performance: 0.95, G Performance: 0.03\n",
            "Epoch [15/100], Step [150/600], D Loss: 0.1089, G Loss: 4.9978\n",
            "Epoch [15/100], Step [300/600], D Loss: 0.4478, G Loss: 6.0326\n",
            "Epoch [15/100], Step [450/600], D Loss: 0.1885, G Loss: 3.9354\n",
            "Epoch [15/100], Step [600/600], D Loss: 0.1080, G Loss: 3.7407\n",
            "---------Epoch [15/100] : D Performance: 0.96, G Performance: 0.05\n",
            "Epoch [16/100], Step [150/600], D Loss: 0.4471, G Loss: 8.0558\n",
            "Epoch [16/100], Step [300/600], D Loss: 0.2491, G Loss: 4.1922\n",
            "Epoch [16/100], Step [450/600], D Loss: 0.1197, G Loss: 4.1035\n",
            "Epoch [16/100], Step [600/600], D Loss: 0.1264, G Loss: 4.1813\n",
            "---------Epoch [16/100] : D Performance: 0.88, G Performance: 0.02\n",
            "Epoch [17/100], Step [150/600], D Loss: 0.3396, G Loss: 3.2993\n",
            "Epoch [17/100], Step [300/600], D Loss: 0.1349, G Loss: 4.4319\n",
            "Epoch [17/100], Step [450/600], D Loss: 0.0997, G Loss: 6.3150\n",
            "Epoch [17/100], Step [600/600], D Loss: 0.2201, G Loss: 3.4886\n",
            "---------Epoch [17/100] : D Performance: 0.89, G Performance: 0.12\n",
            "Epoch [18/100], Step [150/600], D Loss: 0.2223, G Loss: 2.8103\n",
            "Epoch [18/100], Step [300/600], D Loss: 0.3112, G Loss: 4.7286\n",
            "Epoch [18/100], Step [450/600], D Loss: 0.4808, G Loss: 5.0535\n",
            "Epoch [18/100], Step [600/600], D Loss: 0.1971, G Loss: 3.1437\n",
            "---------Epoch [18/100] : D Performance: 0.78, G Performance: 0.02\n",
            "Epoch [19/100], Step [150/600], D Loss: 0.3441, G Loss: 2.9857\n",
            "Epoch [19/100], Step [300/600], D Loss: 0.0812, G Loss: 4.6649\n",
            "Epoch [19/100], Step [450/600], D Loss: 0.0987, G Loss: 3.9400\n",
            "Epoch [19/100], Step [600/600], D Loss: 0.2413, G Loss: 4.7238\n",
            "---------Epoch [19/100] : D Performance: 0.89, G Performance: 0.06\n",
            "Epoch [20/100], Step [150/600], D Loss: 0.1194, G Loss: 3.6617\n",
            "Epoch [20/100], Step [300/600], D Loss: 0.2054, G Loss: 3.9560\n",
            "Epoch [20/100], Step [450/600], D Loss: 0.1551, G Loss: 4.3128\n",
            "Epoch [20/100], Step [600/600], D Loss: 0.2367, G Loss: 5.5215\n",
            "---------Epoch [20/100] : D Performance: 0.88, G Performance: 0.02\n",
            "Epoch [21/100], Step [150/600], D Loss: 0.1040, G Loss: 10.3230\n",
            "Epoch [21/100], Step [300/600], D Loss: 0.4060, G Loss: 6.3546\n",
            "Epoch [21/100], Step [450/600], D Loss: 0.1641, G Loss: 3.4998\n",
            "Epoch [21/100], Step [600/600], D Loss: 0.0410, G Loss: 4.3132\n",
            "---------Epoch [21/100] : D Performance: 0.94, G Performance: 0.01\n",
            "Epoch [22/100], Step [150/600], D Loss: 0.1930, G Loss: 3.2688\n",
            "Epoch [22/100], Step [300/600], D Loss: 0.1654, G Loss: 5.1984\n",
            "Epoch [22/100], Step [450/600], D Loss: 0.1743, G Loss: 3.3110\n",
            "Epoch [22/100], Step [600/600], D Loss: 0.1686, G Loss: 2.2881\n",
            "---------Epoch [22/100] : D Performance: 0.87, G Performance: 0.04\n",
            "Epoch [23/100], Step [150/600], D Loss: 0.1201, G Loss: 3.2682\n",
            "Epoch [23/100], Step [300/600], D Loss: 0.0943, G Loss: 5.4385\n",
            "Epoch [23/100], Step [450/600], D Loss: 0.1688, G Loss: 5.0544\n",
            "Epoch [23/100], Step [600/600], D Loss: 0.1158, G Loss: 4.1752\n",
            "---------Epoch [23/100] : D Performance: 0.95, G Performance: 0.07\n",
            "Epoch [24/100], Step [150/600], D Loss: 0.1860, G Loss: 4.7162\n",
            "Epoch [24/100], Step [300/600], D Loss: 0.0945, G Loss: 4.3243\n",
            "Epoch [24/100], Step [450/600], D Loss: 0.1356, G Loss: 4.1540\n",
            "Epoch [24/100], Step [600/600], D Loss: 0.1488, G Loss: 4.7903\n",
            "---------Epoch [24/100] : D Performance: 0.95, G Performance: 0.06\n",
            "Epoch [25/100], Step [150/600], D Loss: 0.1262, G Loss: 3.7807\n",
            "Epoch [25/100], Step [300/600], D Loss: 0.1434, G Loss: 5.2821\n",
            "Epoch [25/100], Step [450/600], D Loss: 0.1547, G Loss: 5.2953\n",
            "Epoch [25/100], Step [600/600], D Loss: 0.0825, G Loss: 5.3522\n",
            "---------Epoch [25/100] : D Performance: 0.93, G Performance: 0.01\n",
            "Epoch [26/100], Step [150/600], D Loss: 0.1011, G Loss: 4.7469\n",
            "Epoch [26/100], Step [300/600], D Loss: 0.2640, G Loss: 4.1557\n",
            "Epoch [26/100], Step [450/600], D Loss: 0.2209, G Loss: 3.3463\n",
            "Epoch [26/100], Step [600/600], D Loss: 0.2515, G Loss: 3.5169\n",
            "---------Epoch [26/100] : D Performance: 0.84, G Performance: 0.07\n",
            "Epoch [27/100], Step [150/600], D Loss: 0.2339, G Loss: 3.3314\n",
            "Epoch [27/100], Step [300/600], D Loss: 0.1323, G Loss: 2.4293\n",
            "Epoch [27/100], Step [450/600], D Loss: 0.1232, G Loss: 3.9787\n",
            "Epoch [27/100], Step [600/600], D Loss: 0.1877, G Loss: 2.8210\n",
            "---------Epoch [27/100] : D Performance: 0.92, G Performance: 0.10\n",
            "Epoch [28/100], Step [150/600], D Loss: 0.2492, G Loss: 3.0564\n",
            "Epoch [28/100], Step [300/600], D Loss: 0.1688, G Loss: 4.0679\n",
            "Epoch [28/100], Step [450/600], D Loss: 0.1651, G Loss: 4.7666\n",
            "Epoch [28/100], Step [600/600], D Loss: 0.1338, G Loss: 4.3468\n",
            "---------Epoch [28/100] : D Performance: 0.88, G Performance: 0.02\n",
            "Epoch [29/100], Step [150/600], D Loss: 0.1221, G Loss: 3.6663\n",
            "Epoch [29/100], Step [300/600], D Loss: 0.2101, G Loss: 4.6925\n",
            "Epoch [29/100], Step [450/600], D Loss: 0.2436, G Loss: 3.1225\n",
            "Epoch [29/100], Step [600/600], D Loss: 0.2146, G Loss: 4.0967\n",
            "---------Epoch [29/100] : D Performance: 0.85, G Performance: 0.07\n",
            "Epoch [30/100], Step [150/600], D Loss: 0.1562, G Loss: 3.7891\n",
            "Epoch [30/100], Step [300/600], D Loss: 0.1305, G Loss: 3.1688\n",
            "Epoch [30/100], Step [450/600], D Loss: 0.1758, G Loss: 3.2109\n",
            "Epoch [30/100], Step [600/600], D Loss: 0.2491, G Loss: 3.5401\n",
            "---------Epoch [30/100] : D Performance: 0.91, G Performance: 0.06\n",
            "Epoch [31/100], Step [150/600], D Loss: 0.2770, G Loss: 2.4622\n",
            "Epoch [31/100], Step [300/600], D Loss: 0.3236, G Loss: 2.6825\n",
            "Epoch [31/100], Step [450/600], D Loss: 0.2319, G Loss: 3.5501\n",
            "Epoch [31/100], Step [600/600], D Loss: 0.3894, G Loss: 3.0701\n",
            "---------Epoch [31/100] : D Performance: 0.73, G Performance: 0.09\n",
            "Epoch [32/100], Step [150/600], D Loss: 0.2585, G Loss: 3.2258\n",
            "Epoch [32/100], Step [300/600], D Loss: 0.1959, G Loss: 4.1282\n",
            "Epoch [32/100], Step [450/600], D Loss: 0.1725, G Loss: 3.2141\n",
            "Epoch [32/100], Step [600/600], D Loss: 0.2047, G Loss: 3.4915\n",
            "---------Epoch [32/100] : D Performance: 0.84, G Performance: 0.06\n",
            "Epoch [33/100], Step [150/600], D Loss: 0.2416, G Loss: 7.4078\n",
            "Epoch [33/100], Step [300/600], D Loss: 0.1290, G Loss: 4.2241\n",
            "Epoch [33/100], Step [450/600], D Loss: 0.3403, G Loss: 2.7866\n",
            "Epoch [33/100], Step [600/600], D Loss: 0.1915, G Loss: 3.6326\n",
            "---------Epoch [33/100] : D Performance: 0.89, G Performance: 0.09\n",
            "Epoch [34/100], Step [150/600], D Loss: 0.2031, G Loss: 2.6373\n",
            "Epoch [34/100], Step [300/600], D Loss: 0.3025, G Loss: 3.3358\n",
            "Epoch [34/100], Step [450/600], D Loss: 0.0952, G Loss: 5.5337\n",
            "Epoch [34/100], Step [600/600], D Loss: 0.2627, G Loss: 3.9197\n",
            "---------Epoch [34/100] : D Performance: 0.84, G Performance: 0.08\n",
            "Epoch [35/100], Step [150/600], D Loss: 0.3498, G Loss: 5.1779\n",
            "Epoch [35/100], Step [300/600], D Loss: 0.2678, G Loss: 2.5603\n",
            "Epoch [35/100], Step [450/600], D Loss: 0.1828, G Loss: 3.6337\n",
            "Epoch [35/100], Step [600/600], D Loss: 0.1790, G Loss: 3.4450\n",
            "---------Epoch [35/100] : D Performance: 0.88, G Performance: 0.09\n",
            "Epoch [36/100], Step [150/600], D Loss: 0.2024, G Loss: 3.6393\n",
            "Epoch [36/100], Step [300/600], D Loss: 0.2731, G Loss: 3.0329\n",
            "Epoch [36/100], Step [450/600], D Loss: 0.1415, G Loss: 3.1678\n",
            "Epoch [36/100], Step [600/600], D Loss: 0.1661, G Loss: 3.5697\n",
            "---------Epoch [36/100] : D Performance: 0.87, G Performance: 0.05\n",
            "Epoch [37/100], Step [150/600], D Loss: 0.2243, G Loss: 3.0152\n",
            "Epoch [37/100], Step [300/600], D Loss: 0.1877, G Loss: 4.1149\n",
            "Epoch [37/100], Step [450/600], D Loss: 0.3299, G Loss: 3.8604\n",
            "Epoch [37/100], Step [600/600], D Loss: 0.1945, G Loss: 2.5298\n",
            "---------Epoch [37/100] : D Performance: 0.86, G Performance: 0.03\n",
            "Epoch [38/100], Step [150/600], D Loss: 0.1518, G Loss: 3.2526\n",
            "Epoch [38/100], Step [300/600], D Loss: 0.3314, G Loss: 2.9128\n",
            "Epoch [38/100], Step [450/600], D Loss: 0.2944, G Loss: 2.3833\n",
            "Epoch [38/100], Step [600/600], D Loss: 0.1768, G Loss: 3.5107\n",
            "---------Epoch [38/100] : D Performance: 0.80, G Performance: 0.03\n",
            "Epoch [39/100], Step [150/600], D Loss: 0.2904, G Loss: 2.6734\n",
            "Epoch [39/100], Step [300/600], D Loss: 0.2709, G Loss: 3.4325\n",
            "Epoch [39/100], Step [450/600], D Loss: 0.1802, G Loss: 2.9893\n",
            "Epoch [39/100], Step [600/600], D Loss: 0.2155, G Loss: 3.0828\n",
            "---------Epoch [39/100] : D Performance: 0.85, G Performance: 0.08\n",
            "Epoch [40/100], Step [150/600], D Loss: 0.1308, G Loss: 4.0284\n",
            "Epoch [40/100], Step [300/600], D Loss: 0.2179, G Loss: 2.4878\n",
            "Epoch [40/100], Step [450/600], D Loss: 0.1596, G Loss: 3.0742\n",
            "Epoch [40/100], Step [600/600], D Loss: 0.2399, G Loss: 2.9041\n",
            "---------Epoch [40/100] : D Performance: 0.86, G Performance: 0.12\n",
            "Epoch [41/100], Step [150/600], D Loss: 0.1833, G Loss: 3.4648\n",
            "Epoch [41/100], Step [300/600], D Loss: 0.2233, G Loss: 2.6159\n",
            "Epoch [41/100], Step [450/600], D Loss: 0.3343, G Loss: 4.1441\n",
            "Epoch [41/100], Step [600/600], D Loss: 0.2860, G Loss: 1.8195\n",
            "---------Epoch [41/100] : D Performance: 0.86, G Performance: 0.18\n",
            "Epoch [42/100], Step [150/600], D Loss: 0.2116, G Loss: 3.3103\n",
            "Epoch [42/100], Step [300/600], D Loss: 0.3471, G Loss: 2.3195\n",
            "Epoch [42/100], Step [450/600], D Loss: 0.2283, G Loss: 4.1601\n",
            "Epoch [42/100], Step [600/600], D Loss: 0.1882, G Loss: 2.5512\n",
            "---------Epoch [42/100] : D Performance: 0.91, G Performance: 0.11\n",
            "Epoch [43/100], Step [150/600], D Loss: 0.3011, G Loss: 3.5520\n",
            "Epoch [43/100], Step [300/600], D Loss: 0.2555, G Loss: 2.8901\n",
            "Epoch [43/100], Step [450/600], D Loss: 0.2618, G Loss: 2.4874\n",
            "Epoch [43/100], Step [600/600], D Loss: 0.2192, G Loss: 3.1008\n",
            "---------Epoch [43/100] : D Performance: 0.85, G Performance: 0.08\n",
            "Epoch [44/100], Step [150/600], D Loss: 0.3215, G Loss: 3.7119\n",
            "Epoch [44/100], Step [300/600], D Loss: 0.2156, G Loss: 2.6224\n",
            "Epoch [44/100], Step [450/600], D Loss: 0.2295, G Loss: 3.4137\n",
            "Epoch [44/100], Step [600/600], D Loss: 0.2565, G Loss: 3.4767\n",
            "---------Epoch [44/100] : D Performance: 0.85, G Performance: 0.10\n",
            "Epoch [45/100], Step [150/600], D Loss: 0.1865, G Loss: 2.5702\n",
            "Epoch [45/100], Step [300/600], D Loss: 0.2219, G Loss: 2.9108\n",
            "Epoch [45/100], Step [450/600], D Loss: 0.3234, G Loss: 2.5473\n",
            "Epoch [45/100], Step [600/600], D Loss: 0.2193, G Loss: 2.9609\n",
            "---------Epoch [45/100] : D Performance: 0.92, G Performance: 0.21\n",
            "Epoch [46/100], Step [150/600], D Loss: 0.2302, G Loss: 3.4464\n",
            "Epoch [46/100], Step [300/600], D Loss: 0.3669, G Loss: 1.6020\n",
            "Epoch [46/100], Step [450/600], D Loss: 0.2643, G Loss: 1.9631\n",
            "Epoch [46/100], Step [600/600], D Loss: 0.2188, G Loss: 2.9977\n",
            "---------Epoch [46/100] : D Performance: 0.87, G Performance: 0.14\n",
            "Epoch [47/100], Step [150/600], D Loss: 0.3163, G Loss: 2.1315\n",
            "Epoch [47/100], Step [300/600], D Loss: 0.2135, G Loss: 3.3064\n",
            "Epoch [47/100], Step [450/600], D Loss: 0.2171, G Loss: 2.5162\n",
            "Epoch [47/100], Step [600/600], D Loss: 0.2677, G Loss: 2.7253\n",
            "---------Epoch [47/100] : D Performance: 0.82, G Performance: 0.12\n",
            "Epoch [48/100], Step [150/600], D Loss: 0.4034, G Loss: 2.8470\n",
            "Epoch [48/100], Step [300/600], D Loss: 0.2693, G Loss: 2.7150\n",
            "Epoch [48/100], Step [450/600], D Loss: 0.1975, G Loss: 2.9819\n",
            "Epoch [48/100], Step [600/600], D Loss: 0.2998, G Loss: 1.6561\n",
            "---------Epoch [48/100] : D Performance: 0.83, G Performance: 0.20\n",
            "Epoch [49/100], Step [150/600], D Loss: 0.2002, G Loss: 3.0637\n",
            "Epoch [49/100], Step [300/600], D Loss: 0.3141, G Loss: 1.8123\n",
            "Epoch [49/100], Step [450/600], D Loss: 0.3940, G Loss: 2.0139\n",
            "Epoch [49/100], Step [600/600], D Loss: 0.2682, G Loss: 2.2377\n",
            "---------Epoch [49/100] : D Performance: 0.85, G Performance: 0.18\n",
            "Epoch [50/100], Step [150/600], D Loss: 0.2797, G Loss: 2.1630\n",
            "Epoch [50/100], Step [300/600], D Loss: 0.3797, G Loss: 3.5741\n",
            "Epoch [50/100], Step [450/600], D Loss: 0.2853, G Loss: 2.3175\n",
            "Epoch [50/100], Step [600/600], D Loss: 0.3514, G Loss: 2.1256\n",
            "---------Epoch [50/100] : D Performance: 0.84, G Performance: 0.22\n",
            "Epoch [51/100], Step [150/600], D Loss: 0.2515, G Loss: 2.7572\n",
            "Epoch [51/100], Step [300/600], D Loss: 0.3219, G Loss: 2.5611\n",
            "Epoch [51/100], Step [450/600], D Loss: 0.2557, G Loss: 2.9103\n",
            "Epoch [51/100], Step [600/600], D Loss: 0.3447, G Loss: 2.8275\n",
            "---------Epoch [51/100] : D Performance: 0.75, G Performance: 0.13\n",
            "Epoch [52/100], Step [150/600], D Loss: 0.2690, G Loss: 2.6012\n",
            "Epoch [52/100], Step [300/600], D Loss: 0.2341, G Loss: 2.3427\n",
            "Epoch [52/100], Step [450/600], D Loss: 0.3003, G Loss: 2.0036\n",
            "Epoch [52/100], Step [600/600], D Loss: 0.2476, G Loss: 2.6175\n",
            "---------Epoch [52/100] : D Performance: 0.86, G Performance: 0.15\n",
            "Epoch [53/100], Step [150/600], D Loss: 0.3367, G Loss: 1.8517\n",
            "Epoch [53/100], Step [300/600], D Loss: 0.3494, G Loss: 1.9884\n",
            "Epoch [53/100], Step [450/600], D Loss: 0.4165, G Loss: 2.1531\n",
            "Epoch [53/100], Step [600/600], D Loss: 0.2870, G Loss: 2.1745\n",
            "---------Epoch [53/100] : D Performance: 0.83, G Performance: 0.19\n",
            "Epoch [54/100], Step [150/600], D Loss: 0.4425, G Loss: 1.4764\n",
            "Epoch [54/100], Step [300/600], D Loss: 0.3505, G Loss: 2.3163\n",
            "Epoch [54/100], Step [450/600], D Loss: 0.2599, G Loss: 2.3226\n",
            "Epoch [54/100], Step [600/600], D Loss: 0.2283, G Loss: 2.0103\n",
            "---------Epoch [54/100] : D Performance: 0.89, G Performance: 0.17\n",
            "Epoch [55/100], Step [150/600], D Loss: 0.1872, G Loss: 2.4805\n",
            "Epoch [55/100], Step [300/600], D Loss: 0.2951, G Loss: 2.5386\n",
            "Epoch [55/100], Step [450/600], D Loss: 0.2620, G Loss: 3.2333\n",
            "Epoch [55/100], Step [600/600], D Loss: 0.3127, G Loss: 2.0235\n",
            "---------Epoch [55/100] : D Performance: 0.81, G Performance: 0.13\n",
            "Epoch [56/100], Step [150/600], D Loss: 0.3658, G Loss: 2.9408\n",
            "Epoch [56/100], Step [300/600], D Loss: 0.2455, G Loss: 2.2518\n",
            "Epoch [56/100], Step [450/600], D Loss: 0.2594, G Loss: 2.2398\n",
            "Epoch [56/100], Step [600/600], D Loss: 0.2998, G Loss: 2.2799\n",
            "---------Epoch [56/100] : D Performance: 0.78, G Performance: 0.15\n",
            "Epoch [57/100], Step [150/600], D Loss: 0.3429, G Loss: 2.2564\n",
            "Epoch [57/100], Step [300/600], D Loss: 0.3363, G Loss: 2.2345\n",
            "Epoch [57/100], Step [450/600], D Loss: 0.3427, G Loss: 1.9132\n",
            "Epoch [57/100], Step [600/600], D Loss: 0.4520, G Loss: 1.2658\n",
            "---------Epoch [57/100] : D Performance: 0.76, G Performance: 0.20\n",
            "Epoch [58/100], Step [150/600], D Loss: 0.3605, G Loss: 1.9822\n",
            "Epoch [58/100], Step [300/600], D Loss: 0.3602, G Loss: 2.7272\n",
            "Epoch [58/100], Step [450/600], D Loss: 0.3842, G Loss: 1.7823\n",
            "Epoch [58/100], Step [600/600], D Loss: 0.2660, G Loss: 2.3325\n",
            "---------Epoch [58/100] : D Performance: 0.82, G Performance: 0.14\n",
            "Epoch [59/100], Step [150/600], D Loss: 0.3445, G Loss: 1.9460\n",
            "Epoch [59/100], Step [300/600], D Loss: 0.3741, G Loss: 1.8656\n",
            "Epoch [59/100], Step [450/600], D Loss: 0.2660, G Loss: 2.3933\n",
            "Epoch [59/100], Step [600/600], D Loss: 0.4511, G Loss: 2.6408\n",
            "---------Epoch [59/100] : D Performance: 0.68, G Performance: 0.12\n",
            "Epoch [60/100], Step [150/600], D Loss: 0.3374, G Loss: 1.9551\n",
            "Epoch [60/100], Step [300/600], D Loss: 0.2943, G Loss: 1.9974\n",
            "Epoch [60/100], Step [450/600], D Loss: 0.4324, G Loss: 2.8862\n",
            "Epoch [60/100], Step [600/600], D Loss: 0.3326, G Loss: 2.4626\n",
            "---------Epoch [60/100] : D Performance: 0.81, G Performance: 0.17\n",
            "Epoch [61/100], Step [150/600], D Loss: 0.3305, G Loss: 1.9927\n",
            "Epoch [61/100], Step [300/600], D Loss: 0.3319, G Loss: 1.9961\n",
            "Epoch [61/100], Step [450/600], D Loss: 0.3383, G Loss: 1.8788\n",
            "Epoch [61/100], Step [600/600], D Loss: 0.3103, G Loss: 2.4408\n",
            "---------Epoch [61/100] : D Performance: 0.78, G Performance: 0.15\n",
            "Epoch [62/100], Step [150/600], D Loss: 0.3619, G Loss: 2.0408\n",
            "Epoch [62/100], Step [300/600], D Loss: 0.3238, G Loss: 2.2085\n",
            "Epoch [62/100], Step [450/600], D Loss: 0.2250, G Loss: 2.0249\n",
            "Epoch [62/100], Step [600/600], D Loss: 0.3432, G Loss: 1.5631\n",
            "---------Epoch [62/100] : D Performance: 0.79, G Performance: 0.22\n",
            "Epoch [63/100], Step [150/600], D Loss: 0.4147, G Loss: 2.3559\n",
            "Epoch [63/100], Step [300/600], D Loss: 0.2458, G Loss: 2.0944\n",
            "Epoch [63/100], Step [450/600], D Loss: 0.4498, G Loss: 2.1351\n",
            "Epoch [63/100], Step [600/600], D Loss: 0.4582, G Loss: 1.5093\n",
            "---------Epoch [63/100] : D Performance: 0.77, G Performance: 0.28\n",
            "Epoch [64/100], Step [150/600], D Loss: 0.3108, G Loss: 1.9517\n",
            "Epoch [64/100], Step [300/600], D Loss: 0.4017, G Loss: 1.7944\n",
            "Epoch [64/100], Step [450/600], D Loss: 0.3269, G Loss: 1.8351\n",
            "Epoch [64/100], Step [600/600], D Loss: 0.3077, G Loss: 1.7913\n",
            "---------Epoch [64/100] : D Performance: 0.82, G Performance: 0.20\n",
            "Epoch [65/100], Step [150/600], D Loss: 0.3517, G Loss: 1.8320\n",
            "Epoch [65/100], Step [300/600], D Loss: 0.3265, G Loss: 1.8844\n",
            "Epoch [65/100], Step [450/600], D Loss: 0.4250, G Loss: 1.4625\n",
            "Epoch [65/100], Step [600/600], D Loss: 0.3872, G Loss: 3.0107\n",
            "---------Epoch [65/100] : D Performance: 0.74, G Performance: 0.14\n",
            "Epoch [66/100], Step [150/600], D Loss: 0.3324, G Loss: 1.4042\n",
            "Epoch [66/100], Step [300/600], D Loss: 0.3740, G Loss: 1.4817\n",
            "Epoch [66/100], Step [450/600], D Loss: 0.3710, G Loss: 2.6702\n",
            "Epoch [66/100], Step [600/600], D Loss: 0.3099, G Loss: 2.1045\n",
            "---------Epoch [66/100] : D Performance: 0.85, G Performance: 0.25\n",
            "Epoch [67/100], Step [150/600], D Loss: 0.4116, G Loss: 2.2103\n",
            "Epoch [67/100], Step [300/600], D Loss: 0.3506, G Loss: 1.8248\n",
            "Epoch [67/100], Step [450/600], D Loss: 0.3849, G Loss: 1.3103\n",
            "Epoch [67/100], Step [600/600], D Loss: 0.4014, G Loss: 1.9467\n",
            "---------Epoch [67/100] : D Performance: 0.65, G Performance: 0.14\n",
            "Epoch [68/100], Step [150/600], D Loss: 0.3290, G Loss: 1.9552\n",
            "Epoch [68/100], Step [300/600], D Loss: 0.2378, G Loss: 2.1273\n",
            "Epoch [68/100], Step [450/600], D Loss: 0.3688, G Loss: 2.0179\n",
            "Epoch [68/100], Step [600/600], D Loss: 0.2559, G Loss: 2.4354\n",
            "---------Epoch [68/100] : D Performance: 0.84, G Performance: 0.16\n",
            "Epoch [69/100], Step [150/600], D Loss: 0.4215, G Loss: 2.0703\n",
            "Epoch [69/100], Step [300/600], D Loss: 0.3872, G Loss: 2.3898\n",
            "Epoch [69/100], Step [450/600], D Loss: 0.4034, G Loss: 1.5630\n",
            "Epoch [69/100], Step [600/600], D Loss: 0.2664, G Loss: 2.2890\n",
            "---------Epoch [69/100] : D Performance: 0.73, G Performance: 0.10\n",
            "Epoch [70/100], Step [150/600], D Loss: 0.3503, G Loss: 1.7755\n",
            "Epoch [70/100], Step [300/600], D Loss: 0.4242, G Loss: 1.7775\n",
            "Epoch [70/100], Step [450/600], D Loss: 0.4029, G Loss: 2.1564\n",
            "Epoch [70/100], Step [600/600], D Loss: 0.3307, G Loss: 2.1723\n",
            "---------Epoch [70/100] : D Performance: 0.80, G Performance: 0.22\n",
            "Epoch [71/100], Step [150/600], D Loss: 0.3499, G Loss: 2.0480\n",
            "Epoch [71/100], Step [300/600], D Loss: 0.3755, G Loss: 2.3327\n",
            "Epoch [71/100], Step [450/600], D Loss: 0.3253, G Loss: 2.2384\n",
            "Epoch [71/100], Step [600/600], D Loss: 0.3564, G Loss: 1.6543\n",
            "---------Epoch [71/100] : D Performance: 0.75, G Performance: 0.21\n",
            "Epoch [72/100], Step [150/600], D Loss: 0.4043, G Loss: 2.3122\n",
            "Epoch [72/100], Step [300/600], D Loss: 0.3896, G Loss: 1.4448\n",
            "Epoch [72/100], Step [450/600], D Loss: 0.4428, G Loss: 2.0422\n",
            "Epoch [72/100], Step [600/600], D Loss: 0.3615, G Loss: 1.8689\n",
            "---------Epoch [72/100] : D Performance: 0.80, G Performance: 0.22\n",
            "Epoch [73/100], Step [150/600], D Loss: 0.4173, G Loss: 2.0484\n",
            "Epoch [73/100], Step [300/600], D Loss: 0.4465, G Loss: 1.7235\n",
            "Epoch [73/100], Step [450/600], D Loss: 0.3391, G Loss: 2.2051\n",
            "Epoch [73/100], Step [600/600], D Loss: 0.3927, G Loss: 1.8437\n",
            "---------Epoch [73/100] : D Performance: 0.76, G Performance: 0.24\n",
            "Epoch [74/100], Step [150/600], D Loss: 0.4357, G Loss: 1.5798\n",
            "Epoch [74/100], Step [300/600], D Loss: 0.3784, G Loss: 1.9226\n",
            "Epoch [74/100], Step [450/600], D Loss: 0.3760, G Loss: 1.8992\n",
            "Epoch [74/100], Step [600/600], D Loss: 0.3472, G Loss: 1.7470\n",
            "---------Epoch [74/100] : D Performance: 0.82, G Performance: 0.28\n",
            "Epoch [75/100], Step [150/600], D Loss: 0.4614, G Loss: 1.8347\n",
            "Epoch [75/100], Step [300/600], D Loss: 0.3747, G Loss: 1.6694\n",
            "Epoch [75/100], Step [450/600], D Loss: 0.3991, G Loss: 1.6825\n",
            "Epoch [75/100], Step [600/600], D Loss: 0.4952, G Loss: 1.7783\n",
            "---------Epoch [75/100] : D Performance: 0.61, G Performance: 0.19\n",
            "Epoch [76/100], Step [150/600], D Loss: 0.3693, G Loss: 1.9788\n",
            "Epoch [76/100], Step [300/600], D Loss: 0.3273, G Loss: 1.6947\n",
            "Epoch [76/100], Step [450/600], D Loss: 0.4035, G Loss: 2.1474\n",
            "Epoch [76/100], Step [600/600], D Loss: 0.4411, G Loss: 2.1394\n",
            "---------Epoch [76/100] : D Performance: 0.74, G Performance: 0.26\n",
            "Epoch [77/100], Step [150/600], D Loss: 0.3699, G Loss: 1.9188\n",
            "Epoch [77/100], Step [300/600], D Loss: 0.3413, G Loss: 2.0484\n",
            "Epoch [77/100], Step [450/600], D Loss: 0.3926, G Loss: 1.5510\n",
            "Epoch [77/100], Step [600/600], D Loss: 0.6010, G Loss: 1.3045\n",
            "---------Epoch [77/100] : D Performance: 0.69, G Performance: 0.35\n",
            "Epoch [78/100], Step [150/600], D Loss: 0.4196, G Loss: 2.4223\n",
            "Epoch [78/100], Step [300/600], D Loss: 0.4093, G Loss: 1.3852\n",
            "Epoch [78/100], Step [450/600], D Loss: 0.4913, G Loss: 2.0960\n",
            "Epoch [78/100], Step [600/600], D Loss: 0.4678, G Loss: 1.2712\n",
            "---------Epoch [78/100] : D Performance: 0.75, G Performance: 0.29\n",
            "Epoch [79/100], Step [150/600], D Loss: 0.5595, G Loss: 1.5758\n",
            "Epoch [79/100], Step [300/600], D Loss: 0.4662, G Loss: 1.5071\n",
            "Epoch [79/100], Step [450/600], D Loss: 0.3978, G Loss: 1.5618\n",
            "Epoch [79/100], Step [600/600], D Loss: 0.4903, G Loss: 1.6522\n",
            "---------Epoch [79/100] : D Performance: 0.71, G Performance: 0.32\n",
            "Epoch [80/100], Step [150/600], D Loss: 0.4191, G Loss: 1.6806\n",
            "Epoch [80/100], Step [300/600], D Loss: 0.5346, G Loss: 1.4194\n",
            "Epoch [80/100], Step [450/600], D Loss: 0.3946, G Loss: 1.5981\n",
            "Epoch [80/100], Step [600/600], D Loss: 0.4146, G Loss: 1.7813\n",
            "---------Epoch [80/100] : D Performance: 0.74, G Performance: 0.23\n",
            "Epoch [81/100], Step [150/600], D Loss: 0.4293, G Loss: 1.6127\n",
            "Epoch [81/100], Step [300/600], D Loss: 0.4569, G Loss: 2.1768\n",
            "Epoch [81/100], Step [450/600], D Loss: 0.3667, G Loss: 1.9431\n",
            "Epoch [81/100], Step [600/600], D Loss: 0.4180, G Loss: 1.7036\n",
            "---------Epoch [81/100] : D Performance: 0.72, G Performance: 0.23\n",
            "Epoch [82/100], Step [150/600], D Loss: 0.2856, G Loss: 2.3390\n",
            "Epoch [82/100], Step [300/600], D Loss: 0.2566, G Loss: 2.1032\n",
            "Epoch [82/100], Step [450/600], D Loss: 0.3780, G Loss: 1.4352\n",
            "Epoch [82/100], Step [600/600], D Loss: 0.4464, G Loss: 1.2906\n",
            "---------Epoch [82/100] : D Performance: 0.76, G Performance: 0.30\n",
            "Epoch [83/100], Step [150/600], D Loss: 0.4378, G Loss: 1.6833\n",
            "Epoch [83/100], Step [300/600], D Loss: 0.3427, G Loss: 1.6395\n",
            "Epoch [83/100], Step [450/600], D Loss: 0.3103, G Loss: 2.0596\n",
            "Epoch [83/100], Step [600/600], D Loss: 0.3389, G Loss: 1.9244\n",
            "---------Epoch [83/100] : D Performance: 0.74, G Performance: 0.19\n",
            "Epoch [84/100], Step [150/600], D Loss: 0.4201, G Loss: 2.3855\n",
            "Epoch [84/100], Step [300/600], D Loss: 0.3978, G Loss: 1.3027\n",
            "Epoch [84/100], Step [450/600], D Loss: 0.3037, G Loss: 1.9553\n",
            "Epoch [84/100], Step [600/600], D Loss: 0.4440, G Loss: 1.7344\n",
            "---------Epoch [84/100] : D Performance: 0.69, G Performance: 0.23\n",
            "Epoch [85/100], Step [150/600], D Loss: 0.4922, G Loss: 1.6290\n",
            "Epoch [85/100], Step [300/600], D Loss: 0.4285, G Loss: 1.4419\n",
            "Epoch [85/100], Step [450/600], D Loss: 0.5065, G Loss: 1.8556\n",
            "Epoch [85/100], Step [600/600], D Loss: 0.4198, G Loss: 1.8549\n",
            "---------Epoch [85/100] : D Performance: 0.70, G Performance: 0.19\n",
            "Epoch [86/100], Step [150/600], D Loss: 0.3535, G Loss: 1.6747\n",
            "Epoch [86/100], Step [300/600], D Loss: 0.3945, G Loss: 1.4993\n",
            "Epoch [86/100], Step [450/600], D Loss: 0.4717, G Loss: 1.9417\n",
            "Epoch [86/100], Step [600/600], D Loss: 0.3076, G Loss: 1.7204\n",
            "---------Epoch [86/100] : D Performance: 0.84, G Performance: 0.27\n",
            "Epoch [87/100], Step [150/600], D Loss: 0.4638, G Loss: 1.2865\n",
            "Epoch [87/100], Step [300/600], D Loss: 0.4497, G Loss: 1.4797\n",
            "Epoch [87/100], Step [450/600], D Loss: 0.3615, G Loss: 1.6535\n",
            "Epoch [87/100], Step [600/600], D Loss: 0.4210, G Loss: 1.7579\n",
            "---------Epoch [87/100] : D Performance: 0.69, G Performance: 0.22\n",
            "Epoch [88/100], Step [150/600], D Loss: 0.4069, G Loss: 1.6434\n",
            "Epoch [88/100], Step [300/600], D Loss: 0.3654, G Loss: 1.6211\n",
            "Epoch [88/100], Step [450/600], D Loss: 0.3936, G Loss: 1.2345\n",
            "Epoch [88/100], Step [600/600], D Loss: 0.5544, G Loss: 1.5197\n",
            "---------Epoch [88/100] : D Performance: 0.65, G Performance: 0.28\n",
            "Epoch [89/100], Step [150/600], D Loss: 0.4234, G Loss: 1.7062\n",
            "Epoch [89/100], Step [300/600], D Loss: 0.4468, G Loss: 1.8113\n",
            "Epoch [89/100], Step [450/600], D Loss: 0.4334, G Loss: 1.4212\n",
            "Epoch [89/100], Step [600/600], D Loss: 0.3875, G Loss: 1.8285\n",
            "---------Epoch [89/100] : D Performance: 0.69, G Performance: 0.18\n",
            "Epoch [90/100], Step [150/600], D Loss: 0.5139, G Loss: 1.6265\n",
            "Epoch [90/100], Step [300/600], D Loss: 0.4035, G Loss: 1.8225\n",
            "Epoch [90/100], Step [450/600], D Loss: 0.4415, G Loss: 1.8097\n",
            "Epoch [90/100], Step [600/600], D Loss: 0.4128, G Loss: 1.9094\n",
            "---------Epoch [90/100] : D Performance: 0.65, G Performance: 0.16\n",
            "Epoch [91/100], Step [150/600], D Loss: 0.4427, G Loss: 1.6908\n",
            "Epoch [91/100], Step [300/600], D Loss: 0.4617, G Loss: 1.4863\n",
            "Epoch [91/100], Step [450/600], D Loss: 0.4877, G Loss: 1.6685\n",
            "Epoch [91/100], Step [600/600], D Loss: 0.4977, G Loss: 1.3127\n",
            "---------Epoch [91/100] : D Performance: 0.69, G Performance: 0.31\n",
            "Epoch [92/100], Step [150/600], D Loss: 0.4393, G Loss: 1.3881\n",
            "Epoch [92/100], Step [300/600], D Loss: 0.4901, G Loss: 1.2025\n",
            "Epoch [92/100], Step [450/600], D Loss: 0.5285, G Loss: 1.5127\n",
            "Epoch [92/100], Step [600/600], D Loss: 0.6045, G Loss: 1.4629\n",
            "---------Epoch [92/100] : D Performance: 0.60, G Performance: 0.32\n",
            "Epoch [93/100], Step [150/600], D Loss: 0.6209, G Loss: 1.2409\n",
            "Epoch [93/100], Step [300/600], D Loss: 0.3871, G Loss: 1.6202\n",
            "Epoch [93/100], Step [450/600], D Loss: 0.4105, G Loss: 1.4593\n",
            "Epoch [93/100], Step [600/600], D Loss: 0.5240, G Loss: 1.3449\n",
            "---------Epoch [93/100] : D Performance: 0.69, G Performance: 0.34\n",
            "Epoch [94/100], Step [150/600], D Loss: 0.4829, G Loss: 1.3567\n",
            "Epoch [94/100], Step [300/600], D Loss: 0.5999, G Loss: 1.5976\n",
            "Epoch [94/100], Step [450/600], D Loss: 0.6676, G Loss: 1.7613\n",
            "Epoch [94/100], Step [600/600], D Loss: 0.4730, G Loss: 1.6737\n",
            "---------Epoch [94/100] : D Performance: 0.69, G Performance: 0.26\n",
            "Epoch [95/100], Step [150/600], D Loss: 0.5081, G Loss: 1.4447\n",
            "Epoch [95/100], Step [300/600], D Loss: 0.4437, G Loss: 1.5992\n",
            "Epoch [95/100], Step [450/600], D Loss: 0.4346, G Loss: 2.0825\n",
            "Epoch [95/100], Step [600/600], D Loss: 0.4489, G Loss: 1.7412\n",
            "---------Epoch [95/100] : D Performance: 0.72, G Performance: 0.27\n",
            "Epoch [96/100], Step [150/600], D Loss: 0.4719, G Loss: 1.4229\n",
            "Epoch [96/100], Step [300/600], D Loss: 0.4690, G Loss: 1.3165\n",
            "Epoch [96/100], Step [450/600], D Loss: 0.4755, G Loss: 1.6823\n",
            "Epoch [96/100], Step [600/600], D Loss: 0.4808, G Loss: 1.7427\n",
            "---------Epoch [96/100] : D Performance: 0.67, G Performance: 0.26\n",
            "Epoch [97/100], Step [150/600], D Loss: 0.4561, G Loss: 1.5165\n",
            "Epoch [97/100], Step [300/600], D Loss: 0.4844, G Loss: 1.2444\n",
            "Epoch [97/100], Step [450/600], D Loss: 0.4214, G Loss: 2.0715\n",
            "Epoch [97/100], Step [600/600], D Loss: 0.4922, G Loss: 1.3220\n",
            "---------Epoch [97/100] : D Performance: 0.68, G Performance: 0.30\n",
            "Epoch [98/100], Step [150/600], D Loss: 0.4939, G Loss: 1.5883\n",
            "Epoch [98/100], Step [300/600], D Loss: 0.5988, G Loss: 1.1368\n",
            "Epoch [98/100], Step [450/600], D Loss: 0.4539, G Loss: 1.4126\n",
            "Epoch [98/100], Step [600/600], D Loss: 0.4482, G Loss: 1.7343\n",
            "---------Epoch [98/100] : D Performance: 0.73, G Performance: 0.30\n",
            "Epoch [99/100], Step [150/600], D Loss: 0.4660, G Loss: 1.7886\n",
            "Epoch [99/100], Step [300/600], D Loss: 0.6477, G Loss: 1.7931\n",
            "Epoch [99/100], Step [450/600], D Loss: 0.5192, G Loss: 1.4798\n",
            "Epoch [99/100], Step [600/600], D Loss: 0.6073, G Loss: 1.3306\n",
            "---------Epoch [99/100] : D Performance: 0.66, G Performance: 0.36\n",
            "Epoch [100/100], Step [150/600], D Loss: 0.5231, G Loss: 1.0164\n",
            "Epoch [100/100], Step [300/600], D Loss: 0.4875, G Loss: 1.5676\n",
            "Epoch [100/100], Step [450/600], D Loss: 0.4558, G Loss: 1.7556\n",
            "Epoch [100/100], Step [600/600], D Loss: 0.3963, G Loss: 1.4868\n",
            "---------Epoch [100/100] : D Performance: 0.77, G Performance: 0.32\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader): # 데이터 로더에서 배치 단위로 이미지 가져오기\n",
        "        real_labels = torch.ones(batch_size, 1).to(device) # 실제 이미지에 대한 레이블 생성 (1)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device) # 가짜 이미지에 대한 레이블 생성 (0)\n",
        "\n",
        "        real_images = images.reshape(batch_size, -1).to(device) # 이미지를 1차원으로 변환하여 GPU로 이동\n",
        "\n",
        "        # Generator 훈련\n",
        "        g_optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, noise_size).to(device) # 랜덤 노이즈 생성\n",
        "        fake_images = generator(z) # Generator로 가짜 이미지 생성\n",
        "\n",
        "        g_loss = criterion(discriminator(fake_images), real_labels) # Generator의 손실 계산\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Discriminator 훈련\n",
        "        d_optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, noise_size).to(device) # 랜덤 노이즈 생성\n",
        "        fake_images = generator(z) # Generator로 가짜 이미지 생성\n",
        "\n",
        "        real_loss = criterion(discriminator(real_images), real_labels) # 실제 이미지에 대한 손실 계산\n",
        "        fake_loss = criterion(discriminator(fake_images.detach()), fake_labels) # 가짜 이미지에 대한 손실 계산\n",
        "        d_loss = (real_loss + fake_loss) / 2 # Discriminator의 손실 계산\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # 150 스텝마다 손실 출력\n",
        "        if (i + 1) % 150 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(data_loader)}], \"\n",
        "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # 생성된 가짜 이미지를 저장\n",
        "    save_image(fake_images.view(batch_size, 1, 28, 28),\n",
        "               os.path.join(dir_name, f'GAN_fake_image_{epoch + 1}.png'))\n",
        "\n",
        "    # Discriminator의 성능 평가\n",
        "    d_performance = discriminator(real_images).mean().item()\n",
        "    g_performance = discriminator(fake_images).mean().item()\n",
        "    print(f\"---------Epoch [{epoch + 1}/{num_epochs}] : D Performance: {d_performance:.2f}, G Performance: {g_performance:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYFHDcSxw8qo"
      },
      "source": [
        "#1-2\n",
        "\n",
        "아래 마크다운으로 GAN_fake_image_1.png와 GAN_fake_image_100.png를 함께 첨부해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![GAN_fake_image_1.png](GAN_fake_image_1.png)\n",
        "![GAN_fake_image_100.png](GAN_fake_image_100.png)"
      ],
      "metadata": {
        "id": "_ZMadhQRiGVe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}