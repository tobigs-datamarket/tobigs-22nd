{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daanbee/tobigs-22nd/blob/main/%ED%88%AC%EB%B9%85%EC%8A%A4_Week5_%EA%B3%BC%EC%A0%9C1_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:"
      ],
      "metadata": {
        "id": "ojf8hcSd9hLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KyObPG9Z7v",
        "outputId": "b9d9aade-6f71-4095-a5bd-e2007938c22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê°„ë‹¨í•œ ë¬¸ì¥: ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']\n",
        "words = ['ë‚˜ëŠ”', 'ì‚¬ê³¼ë¥¼', 'ë¨¹ì—ˆë‹¤']  # ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "# ê° ë‹¨ì–´ì— ëŒ€í•œ ì„ì˜ì˜ ë²¡í„° í‘œí˜„ (Word Embedding)\n",
        "word_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0, 0]),   # 'ë‚˜ëŠ”'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1, 0]),  # 'ì‚¬ê³¼ë¥¼'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 0, 1])   # 'ë¨¹ì—ˆë‹¤'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "}\n",
        "\n",
        "# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (Self-Attentionì˜ í•µì‹¬ ì—°ì‚°: Queryì™€ Key ê°„ ë‚´ì )\n",
        "def self_attention(query, key):\n",
        "    return np.dot(query, key)  # Queryì™€ Keyì˜ ë²¡í„° ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "\n",
        "# Attention ê°’ì„ ë‹´ì„ í–‰ë ¬ ì´ˆê¸°í™” (ë‹¨ì–´ ê°œìˆ˜ x ë‹¨ì–´ ê°œìˆ˜ í¬ê¸°)\n",
        "attention_matrix = np.zeros((len(words), len(words)))\n",
        "\n",
        "# ê° ë‹¨ì–´ ê°„ì˜ Self-Attention ê³„ì‚° (Queryì™€ Keyë¥¼ ë¹„êµí•˜ì—¬ Attention ê°’ ì €ì¥)\n",
        "for i in range(len(words)):  # ië²ˆì§¸ ë‹¨ì–´ë¥¼ Queryë¡œ ì„¤ì •\n",
        "    for j in range(len(words)):  # jë²ˆì§¸ ë‹¨ì–´ë¥¼ Keyë¡œ ì„¤ì •\n",
        "        attention_matrix[i][j] = self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        # ië²ˆì§¸ ë‹¨ì–´(Query)ì™€ jë²ˆì§¸ ë‹¨ì–´(Key)ì˜ ìœ ì‚¬ë„ë¥¼ í–‰ë ¬ì— ì €ì¥\n",
        "\n",
        "# Self-Attention í–‰ë ¬ ì¶œë ¥\n",
        "print(\"Self-Attention Matrix:\")\n",
        "print(attention_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:"
      ],
      "metadata": {
        "id": "WA3NEBQC-Dpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—¬ëŸ¬ ê°œì˜ attention headsë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
        "def multi_head_self_attention(query, key, heads=3):\n",
        "    return [np.dot(query, key) for _ in range(heads)]  # Queryì™€ Keyì˜ ë²¡í„° ë‚´ì ì„ heads ê°œìˆ˜ë§Œí¼ ê³„ì‚°í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "\n",
        "# 3ê°œì˜ Attention Headsë¥¼ ê°€ì§€ëŠ” Multi-Head Attention í–‰ë ¬ ì´ˆê¸°í™”\n",
        "multi_head_attention_matrix = np.zeros((len(words), len(words), 3))  # (ë‹¨ì–´ ê°œìˆ˜, ë‹¨ì–´ ê°œìˆ˜, heads ê°œìˆ˜) í¬ê¸°ì˜ 3D í–‰ë ¬\n",
        "\n",
        "# ê° ë‹¨ì–´ ìŒì— ëŒ€í•´ Multi-Head Self-Attention ê³„ì‚°\n",
        "for i in range(len(words)):  # ië²ˆì§¸ ë‹¨ì–´ë¥¼ Queryë¡œ ì„¤ì •\n",
        "    for j in range(len(words)):  # jë²ˆì§¸ ë‹¨ì–´ë¥¼ Keyë¡œ ì„¤ì •\n",
        "        multi_head_attention_matrix[i][j] = multi_head_self_attention(word_vectors[words[i]], word_vectors[words[j]])\n",
        "        # ië²ˆì§¸ ë‹¨ì–´(Query)ì™€ jë²ˆì§¸ ë‹¨ì–´(Key)ì— ëŒ€í•´ ê° headë³„ attention ê°’ì„ í–‰ë ¬ì— ì €ì¥\n",
        "\n",
        "# Multi-Head Self-Attention í–‰ë ¬ ì¶œë ¥\n",
        "print(\"\\nMulti-Head Self-Attention Matrix:\")\n",
        "print(multi_head_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1bvP6lC-efR",
        "outputId": "fdf5f51a-73c7-4290-c72a-fbbaeac85160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Self-Attention Matrix:\n",
            "[[[1. 1. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [1. 1. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Masked Multi-Head Self AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Masked Multi-Head Self Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:"
      ],
      "metadata": {
        "id": "lHm1Y03S-hz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë§ˆìŠ¤í¬ëœ Attentionì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ (í˜„ì¬ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ)\n",
        "def masked_attention(query, key, mask):\n",
        "    return np.dot(query, key) * mask  # Queryì™€ Keyì˜ ë‚´ì  ê²°ê³¼ì— maskë¥¼ ê³±í•´ ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ ì œì™¸\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ë°°ì—´ (ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” ë³´ì§€ë§Œ, ì„¸ ë²ˆì§¸ ë‹¨ì–´ëŠ” ë¬´ì‹œ)\n",
        "mask = np.array([1, 1, 0])  # ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” ê³„ì‚°, ì„¸ ë²ˆì§¸ ë‹¨ì–´ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŒ\n",
        "\n",
        "# ë§ˆìŠ¤í¬ëœ Self-Attention í–‰ë ¬ ì´ˆê¸°í™”\n",
        "masked_attention_matrix = np.zeros((len(words), len(words)))  # ë‹¨ì–´ ê°œìˆ˜ x ë‹¨ì–´ ê°œìˆ˜ í¬ê¸°ì˜ 2D í–‰ë ¬\n",
        "\n",
        "# ê° ë‹¨ì–´ ìŒì— ëŒ€í•´ Masked Self-Attention ê³„ì‚°\n",
        "for i in range(len(words)):  # ië²ˆì§¸ ë‹¨ì–´ë¥¼ Queryë¡œ ì„¤ì •\n",
        "    for j in range(len(words)):  # jë²ˆì§¸ ë‹¨ì–´ë¥¼ Keyë¡œ ì„¤ì •\n",
        "        masked_attention_matrix[i][j] = masked_attention(word_vectors[words[i]], word_vectors[words[j]], mask[j])\n",
        "        # jë²ˆì§¸ ë‹¨ì–´(Key)ì— ëŒ€í•´ ë§ˆìŠ¤í¬ ê°’ì„ ì ìš©í•˜ì—¬ attention ê°’ì„ ê³„ì‚°\n",
        "\n",
        "# ë§ˆìŠ¤í¬ëœ Self-Attention í–‰ë ¬ ì¶œë ¥\n",
        "print(\"\\nMasked Self-Attention Matrix:\")\n",
        "print(masked_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hELE_Nyf-pOP",
        "outputId": "205f1774-febf-4e33-ee26-ef08e87e66ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Masked Self-Attention Matrix:\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross AttentionğŸ“Œ**\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜í–‰í•´ë³´ê³ , Cross Attentionì€ ì–´ë–¤ ë©”ì»¤ë‹ˆì¦˜ì¸ì§€ ì„¤ëª…í•˜ê³ ,\n",
        "\n",
        " ê·¸ ì„¤ëª…ì— ë§ê²Œ ê° ì½”ë“œì— ì§ì ‘ ì£¼ì„ì„ ë‹¬ì•„ë´…ì‹œë‹¤.\n",
        "\n",
        " âœ…ì„¤ëª…:"
      ],
      "metadata": {
        "id": "wEiAlmYi-xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì…ë ¥ ë¬¸ì¥ê³¼ ì‘ë‹µ ë¬¸ì¥ì— í¬í•¨ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "question_words = ['ë„ˆëŠ”', 'ì‚¬ê³¼ë¥¼']  # ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "answer_words = ['ë‚˜ëŠ”', 'ë¨¹ì—ˆë‹¤']   # ì‘ë‹µì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "# ì§ˆë¬¸ ë¬¸ì¥ì— ìˆëŠ” ê° ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ (Word Embedding)\n",
        "question_vectors = {\n",
        "    'ë„ˆëŠ”': np.array([1, 0]),   # 'ë„ˆëŠ”'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "    'ì‚¬ê³¼ë¥¼': np.array([0, 1])  # 'ì‚¬ê³¼ë¥¼'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "}\n",
        "\n",
        "# ì‘ë‹µ ë¬¸ì¥ì— ìˆëŠ” ê° ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ (Word Embedding)\n",
        "answer_vectors = {\n",
        "    'ë‚˜ëŠ”': np.array([1, 0]),   # 'ë‚˜ëŠ”'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "    'ë¨¹ì—ˆë‹¤': np.array([0, 1])  # 'ë¨¹ì—ˆë‹¤'ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°\n",
        "}\n",
        "\n",
        "# Cross-Attention í–‰ë ¬ ì´ˆê¸°í™” (ì§ˆë¬¸ ë‹¨ì–´ ê°œìˆ˜ x ì‘ë‹µ ë‹¨ì–´ ê°œìˆ˜ í¬ê¸°ì˜ í–‰ë ¬)\n",
        "cross_attention_matrix = np.zeros((len(question_words), len(answer_words)))\n",
        "\n",
        "# ì§ˆë¬¸ ë‹¨ì–´ì™€ ì‘ë‹µ ë‹¨ì–´ ì‚¬ì´ì˜ Cross-Attention ê³„ì‚°\n",
        "for i in range(len(question_words)):  # ië²ˆì§¸ ì§ˆë¬¸ ë‹¨ì–´ë¥¼ Queryë¡œ ì„¤ì •\n",
        "    for j in range(len(answer_words)):  # jë²ˆì§¸ ì‘ë‹µ ë‹¨ì–´ë¥¼ Keyë¡œ ì„¤ì •\n",
        "        # ì§ˆë¬¸ ë‹¨ì–´ì˜ ë²¡í„°ì™€ ì‘ë‹µ ë‹¨ì–´ì˜ ë²¡í„° ê°„ ë‚´ì ì„ í†µí•´ Cross-Attention ê°’ì„ ê³„ì‚°\n",
        "        cross_attention_matrix[i][j] = np.dot(question_vectors[question_words[i]], answer_vectors[answer_words[j]])\n",
        "\n",
        "# Cross-Attention í–‰ë ¬ ì¶œë ¥\n",
        "print(\"\\nCross-Attention Matrix:\")\n",
        "print(cross_attention_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2871Q9r-5ZP",
        "outputId": "9cbef301-52d9-4a13-8ac4-ab95a1c9ccd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Attention Matrix:\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}